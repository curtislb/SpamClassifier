From r-help-bounces@stat.math.ethz.ch  Tue May  8 08:37:38 2007
Return-Path: <r-help-bounces@stat.math.ethz.ch>
Received: from hypatia.math.ethz.ch (hypatia.math.ethz.ch [129.132.145.15])
	by flax9.uwaterloo.ca (8.12.8/8.12.5) with ESMTP id l48CbbqD015150
	for <ktwarwic@flax9.uwaterloo.ca>; Tue, 8 May 2007 08:37:38 -0400
Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l48CaHxF022734;
	Tue, 8 May 2007 14:36:36 +0200
X-Spam-Checker-Version: SpamAssassin 3.1.8 (2007-02-13) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, score=0.1 required=5.0 tests=AWL,
	BAYES_50 autolearn=no version=3.1.8
Received: from an-out-0708.google.com (an-out-0708.google.com [209.85.132.240])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l48CaB5V022716
	for <r-help@stat.math.ethz.ch>; Tue, 8 May 2007 14:36:11 +0200
Received: by an-out-0708.google.com with SMTP id c31so292771anc
	for <r-help@stat.math.ethz.ch>; Tue, 08 May 2007 05:36:11 -0700 (PDT)
Received: by 10.100.138.2 with SMTP id l2mr5626077and.1178627767665;
	Tue, 08 May 2007 05:36:07 -0700 (PDT)
Received: by 10.100.198.8 with HTTP; Tue, 8 May 2007 05:36:07 -0700 (PDT)
Message-ID: <f8e6ff050705080536t2e94e13fradde09928adfbe61@mail.gmail.com>
Date: Tue, 8 May 2007 14:36:07 +0200
From: "hadley wickham" <h.wickham@gmail.com>
To: "Adaikalavan Ramasamy" <ramasamy@cancer.org.uk>
In-Reply-To: <46406A32.5080309@cancer.org.uk>
MIME-Version: 1.0
Content-Disposition: inline
References: <f8e6ff050705080208w6e6892dfw9cdff386aa65d573@mail.gmail.com>
	<46406A32.5080309@cancer.org.uk>
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
Cc: R Help <r-help@stat.math.ethz.ch>
Subject: Re: [R] Weighted least squares
X-BeenThere: r-help@stat.math.ethz.ch
X-Mailman-Version: 2.1.9
Precedence: list
List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=unsubscribe>
List-Archive: <https://stat.ethz.ch/pipermail/r-help>
List-Post: <mailto:r-help@stat.math.ethz.ch>
List-Help: <mailto:r-help-request@stat.math.ethz.ch?subject=help>
List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Sender: r-help-bounces@stat.math.ethz.ch
Errors-To: r-help-bounces@stat.math.ethz.ch

On 5/8/07, Adaikalavan Ramasamy <ramasamy@cancer.org.uk> wrote:
> See below.
>
> hadley wickham wrote:
> > Dear all,
> >
> > I'm struggling with weighted least squares, where something that I had
> > assumed to be true appears not to be the case.  Take the following
> > data set as an example:
> >
> > df <- data.frame(x = runif(100, 0, 100))
> > df$y <- df$x + 1 + rnorm(100, sd=15)
> >
> > I had expected that:
> >
> > summary(lm(y ~ x, data=df, weights=rep(2, 100)))
> > summary(lm(y ~ x, data=rbind(df,df)))
>
> You assign weights to different points according to some external
> quality or reliability measure not number of times the data point was
> measured.

That is one type of weighting - but what if I have already aggregated
data?  That is a perfectly valid type of weighting too.

> Look at the estimates and standard error of the two models below:
>
>   coefficients( summary(f.w <- lm(y ~ x, data=df, weights=rep(2, 100))) )
>               Estimate Std. Error   t value     Pr(>|t|)
>   (Intercept) 1.940765 3.30348066  0.587491 5.582252e-01
>   x           0.982610 0.05893262 16.673448 2.264258e-30
>
>   coefficients( summary( f.u <- lm(y ~ x, data=rbind(df,df) ) ) )
>               Estimate Std. Error    t value     Pr(>|t|)
>   (Intercept) 1.940765 2.32408609  0.8350659 4.046871e-01
>   x           0.982610 0.04146066 23.6998165 1.012067e-59
>
> You can see that they have same coefficient estimates but the second one
>   has smaller variances.
>
> The repeated values artificially deflates the variance and thus inflates
> the precision. This is why you cannot treat replicate data as
> independent observations.

Hardly artificially - I have repeated observations.

> > would be equivalent, but they are not.  I suspect the difference is
> > how the degrees of freedom is calculated - I had expected it to be
> > sum(weights), but seems to be sum(weights > 0).  This seems
> > unintuitive to me:
> >
> > summary(lm(y ~ x, data=df, weights=rep(c(0,2), each=50)))
> > summary(lm(y ~ x, data=df, weights=rep(c(0.01,2), each=50)))
> >
> > What am I missing?  And what is the usual way to do a linear
> > regression when you have aggregated data?
>
> I would be best to use the individual data points instead of aggregated
> data as it allows you to estimate the within-group variations as well.

There is no within group variation - these are observations that occur
with same values many times in the dataset, so have been aggregated
into the a contingency table-like format.

> If you had individual data points, you could try something as follows.
> Please check the codes as I am no expert in the area of repeated measures.
>
>   x  <- runif(100, 0, 100)
>   y1 <- x + rnorm(100, mean=1, sd=15)
>   y2 <- y1 + rnorm(100, sd=5)
>
>   df <- data.frame( y=c(y1, y2),
>                     x=c(x,x),
>                     subject=factor(rep( paste("p", 1:100, sep=""), 2 ) ))
>
>   library(nlme)
>   summary( lme( y ~ x, random = ~ 1 | subject, data=df ) )
>
> Try reading Pinheiro and Bates (http://tinyurl.com/yvvrr7) or related
> material for more information. Hope this helps.

I'm not interested in a mixed model, and I don't have individual data points.

Hadley

______________________________________________
R-help@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

