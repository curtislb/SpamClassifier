From perl6-all-return-80969-ktwarwic=speedy.uwaterloo.ca@perl.org  Sat Apr 28 11:34:32 2007
Return-Path: <perl6-all-return-80969-ktwarwic=speedy.uwaterloo.ca@perl.org>
Received: from lists.develooper.com (x6.develooper.com [63.251.223.186])
	by speedy.uwaterloo.ca (8.12.8/8.12.5) with SMTP id l3SFYN6w009065
	for <ktwarwic@speedy.uwaterloo.ca>; Sat, 28 Apr 2007 11:34:24 -0400
Received: (qmail 32557 invoked by uid 514); 28 Apr 2007 15:34:12 -0000
Mailing-List: contact perl6-all-help@perl.org; run by ezmlm
Precedence: bulk
List-Post: <mailto:perl6-all@perl.org>
List-Help: <mailto:perl6-all-help@perl.org>
List-Unsubscribe: <mailto:perl6-all-unsubscribe@perl.org>
List-Subscribe: <mailto:perl6-all-subscribe@perl.org>
List-Id: <perl6-all.perl.org>
Delivered-To: mailing list perl6-all@perl.org
Received: (qmail 32544 invoked from network); 28 Apr 2007 15:34:12 -0000
Delivered-To: perl6-all-poster@perl.org
X-Spam-Status: No, hits=-2.6 required=8.0
	tests=BAYES_00,DK_POLICY_SIGNSOME
X-Spam-Check-By: la.mx.develooper.com
Received-SPF: pass (x1.develooper.com: local policy)
X-Mailing-List: contact perl6-internals-help@perl.org; run by ezmlm
X-Mailing-List-Name: perl6-internals
List-Id: <perl6-internals.perl.org>
Delivered-To: mailing list perl6-internals@perl.org
Delivered-To: moderator for perl6-internals@perl.org
Delivered-To: perl6-internals@perl.org
Received-SPF: pass (x1.develooper.com: local policy)
From: Shawn M Moore (via RT) <parrotbug-followup@parrotcode.org>
X-RT-NewTicket: yes
To: bugs-bitbucket@netlabs.develooper.com
Resent-To: perl6-internals@perl.org
Mail-Followup-To: perl6-internals@perl.org
Reply-To: perl6-internals@perl.org
Date: Sat, 28 Apr 2007 02:54:32 -0700
Subject: [perl #42785] [PATCH] Improve t/codingstd/c_indent.t, fix newly-failing tests 
In-Reply-To: <e7e9b6ab0704280253r19abc9fau66506490a663a9a6@mail.gmail.com>
References: <RT-Ticket-42785@perl.org> <e7e9b6ab0704280253r19abc9fau66506490a663a9a6@mail.gmail.com>
Message-ID: <rt-3.6.HEAD-30201-1177754072-1993.42785-72-0@perl.org>
X-RT-Loop-Prevention: perl
RT-Ticket: perl #42785
Managed-by: RT 3.6.HEAD (http://www.bestpractical.com/rt/)
RT-Originator: sartak@gmail.com
MIME-Version: 1.0
X-RT-Original-Encoding: utf-8
Content-type: multipart/mixed; boundary="----------=_1177754072-30201-220"
Resent-Message-Id: <20070428095433.B08EB2AFD8@x3.develooper.com>
Resent-Date: Sat, 28 Apr 2007 02:54:33 -0700 (PDT)
Resent-From: rt-sartak=gmail.com@netlabs.develooper.com
X-Virus-Checked: Checked
X-Virus-Checked: Checked
X-Old-Spam-Check-By: la.mx.develooper.com
X-Old-Spam-Status: No, hits=-4.4 required=8.0
	tests=ALL_TRUSTED,BAYES_00,DK_POLICY_SIGNSOME
Status: O
Content-Length: 119519
Lines: 3533

------------=_1177754072-30201-220
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 8bit

# New Ticket Created by  Shawn M Moore 
# Please include the string:  [perl #42785]
# in the subject line of all future correspondence about this issue. 
# <URL: http://rt.perl.org/rt3/Ticket/Display.html?id=42785 >


Hiya,

As it stands, c_indent.t is pretty simple (it even admits so in an
internal comment). The way it detects misindented code is
(essentially) by checking the first line after an {-at-EOL and "assume
that more likely than not indenting is consistent within a func body."
Which is fine, but it doesn't catch typos or what have you.

So I added a trick to it: It'll remember the last character of the
previous line and ignore the indentation of the current line if that
previous line's last character was anything but a ;. This isn't
perfect; originally I had it the other way around (ignore if the last
character is a &, |, +, etc. but that ended up containing pretty much
everything but ';'). So if the previous line's last character was a ;,
it'll test that the indentation of the current line is divisible by
four. I'd check if the current line's indentation is equal to the
previous line's indentation, but you run into problems with braceless
if-statements, and other things.

Some other small changes include ignoring multiline comments, except
for the first line (this is for coda and weird comments), making the
test-failure diags less ambiguous (it's currently a little whacky)

While the older version of c_indent.t passed everything, this improved
version catches a number of errors. So also attached is a patch that
corrects the problems the new c_indent.t caught. Of course, there are
many problems this test script doesn't catch (but which my earlier
revisions of the script did catch, but also hit a lot of false
positives) and I'll work those out maybe tomorrow.

c_indent.diff
 c_indent.t |   34 ++++++++++++++++++++++++++++++----
 1 file changed, 30 insertions(+), 4 deletions(-)

c_indent-fixes.diff
 config/gen/platform/cygwin/math.c  |   26
 config/gen/platform/generic/math.c |   26
 config/gen/platform/netbsd/math.c  |   26
 config/gen/platform/openbsd/math.c |   26
 config/gen/platform/solaris/math.c |   26
 src/debug.c                        |    2
 src/exec_save.c                    |    2
 src/jit/ppc/jit_emit.h             |    2
 src/jit/sun4/jit_emit.h            |    4
 src/malloc.c                       | 2585 ++++++++++++++++++-------------------
 src/mmd.c                          |    4
 11 files changed, 1369 insertions(+), 1360 deletions(-)

Shawn M Moore

------------=_1177754072-30201-220
Content-Type: text/plain; charset="ANSI_X3.4-1968"; name="c_indent.diff"
Content-Disposition: inline; filename="c_indent.diff"
Content-Transfer-Encoding: 7bit
RT-Attachment: 42785/254338/112077

Index: t/codingstd/c_indent.t
===================================================================
--- t/codingstd/c_indent.t	(revision 18348)
+++ t/codingstd/c_indent.t	(working copy)
@@ -56,12 +56,25 @@
         @source = <$fh>;
 
         my @stack;
-        my $line = 0;
-        my $f    = undef;
+        my $line           = 0;
+        my $f              = undef;
+        my $prev_last_char = '';
+        my $last_char      = '';
+        my $in_comment     = 0;
+
         foreach (@source) {
             $line++;
             next unless defined $_;
+            chomp;
 
+            $prev_last_char = $last_char;
+            $last_char = substr($_, -1, 1);
+
+            # ignore multi-line comments (except the first line)
+            $in_comment = 0, next if $in_comment && m{\*/} && $' !~ m{/\*};
+            next if $in_comment;
+            $in_comment = 1 if m{/\*} && $' !~ m{\*/};
+
             ## preprocessor scan
             if (/^\s*\#(\s*)(ifndef|ifdef|if)\s+(.*)/) {
                 next if (/PARROT_IN_CORE|_GUARD/);
@@ -113,6 +126,7 @@
                         . ( join ' > ', @stack ) . "\n";
                     $pp_failed{"$path\n"} = 1;
                 }
+                next;
             }
 
             ## c source scan
@@ -144,14 +158,26 @@
                     my ($indent) = /^(\s*)/;
                     if ( length($indent) != 4 ) {
                         push @c_indent => "$path:$line\n"
-                            . "apparent non-4 space indenting ("
+                            . "    apparent non-4 space indenting ("
                             . length($indent)
-                            . " spaces)";
+                            . " spaces)\n";
                         $c_failed{"$path\n"} = 1;
                     }
                 }
                 $f = undef;
             }
+
+            my ($indent) = /^(\s+)/ or next;
+            $indent = length($indent);
+
+            if ($indent % 4 && !$in_comment && $prev_last_char eq ';')
+            {
+                push @c_indent => "$path:$line\n"
+                    . "    apparent non-4 space indenting ($indent space"
+                    . ($indent == 1 ? '' : 's')
+                    . ")\n";
+                $c_failed{"$path\n"} = 1;
+            }
         }
     }
 

------------=_1177754072-30201-220
Content-Type: text/plain; charset="ANSI_X3.4-1968"; name="c_indent-fixes.diff"
Content-Disposition: inline; filename="c_indent-fixes.diff"
Content-Transfer-Encoding: 7bit
RT-Attachment: 42785/254338/112078

Index: src/exec_save.c
===================================================================
--- src/exec_save.c	(revision 18348)
+++ src/exec_save.c	(working copy)
@@ -65,7 +65,7 @@
     header.a_drsize = obj->data_rellocation_count
         * sizeof (struct relocation_info);
     save_struct(fp, &header, sizeof (struct exec));
-   /* Text */
+    /* Text */
     for (i = 0; i < obj->text.size; i++)
         fprintf(fp, "%c", obj->text.code[i]);
     /* Data */
Index: src/debug.c
===================================================================
--- src/debug.c	(revision 18348)
+++ src/debug.c	(working copy)
@@ -1987,7 +1987,7 @@
         run = eval_pf->cur_cs->base.data;
         DO_OP(run,interp);
         Parrot_switch_to_cs(interp, old_cs, 1);
-       /* TODO destroy packfile */
+        /* TODO destroy packfile */
     }
 #endif
 
Index: src/malloc.c
===================================================================
--- src/malloc.c	(revision 18348)
+++ src/malloc.c	(working copy)
@@ -2945,28 +2945,28 @@
 
         front_misalign = (INTERNAL_SIZE_T)chunk2mem(mm) & MALLOC_ALIGN_MASK;
         if (front_misalign > 0) {
-          correction = MALLOC_ALIGNMENT - front_misalign;
-          p = (mchunkptr)(mm + correction);
-          p->prev_size = correction;
-          set_head(p, (size - correction) |IS_MMAPPED);
+            correction = MALLOC_ALIGNMENT - front_misalign;
+            p = (mchunkptr)(mm + correction);
+            p->prev_size = correction;
+            set_head(p, (size - correction) |IS_MMAPPED);
         }
         else {
-          p = (mchunkptr)mm;
-          p->prev_size = 0;
-          set_head(p, size|IS_MMAPPED);
+            p = (mchunkptr)mm;
+            p->prev_size = 0;
+            set_head(p, size|IS_MMAPPED);
         }
 
         /* update statistics */
 
         if (++av->n_mmaps > av->max_n_mmaps)
-          av->max_n_mmaps = av->n_mmaps;
+            av->max_n_mmaps = av->n_mmaps;
 
         sum = av->mmapped_mem += size;
         if (sum > (CHUNK_SIZE_T)(av->max_mmapped_mem))
-          av->max_mmapped_mem = sum;
+            av->max_mmapped_mem = sum;
         sum += av->sbrked_mem;
         if (sum > (CHUNK_SIZE_T)(av->max_total_mem))
-          av->max_total_mem = sum;
+            av->max_total_mem = sum;
 
         check_chunk(p);
 
@@ -2976,315 +2976,314 @@
   }
 #endif
 
-  /* Record incoming configuration of top */
+    /* Record incoming configuration of top */
 
-  old_top  = av->top;
-  old_size = chunksize(old_top);
-  old_end  = (char*)(chunk_at_offset(old_top, old_size));
+    old_top  = av->top;
+    old_size = chunksize(old_top);
+    old_end  = (char*)(chunk_at_offset(old_top, old_size));
 
-  brk = snd_brk = (char*)(MORECORE_FAILURE);
+    brk = snd_brk = (char*)(MORECORE_FAILURE);
 
-  /*
-     If not the first time through, we require old_size to be
-     at least MINSIZE and to have prev_inuse set.
-  */
+    /*
+       If not the first time through, we require old_size to be
+       at least MINSIZE and to have prev_inuse set.
+    */
 
-  assert((old_top == initial_top(av) && old_size == 0) ||
-         ((CHUNK_SIZE_T) (old_size) >= MINSIZE &&
-          prev_inuse(old_top)));
+    assert((old_top == initial_top(av) && old_size == 0) ||
+           ((CHUNK_SIZE_T) (old_size) >= MINSIZE &&
+            prev_inuse(old_top)));
 
-  /* Precondition: not enough current space to satisfy nb request */
-  assert((CHUNK_SIZE_T)(old_size) < (CHUNK_SIZE_T)(nb + MINSIZE));
+    /* Precondition: not enough current space to satisfy nb request */
+    assert((CHUNK_SIZE_T)(old_size) < (CHUNK_SIZE_T)(nb + MINSIZE));
 
-  /* Precondition: all fastbins are consolidated */
-  assert(!have_fastchunks(av));
+    /* Precondition: all fastbins are consolidated */
+    assert(!have_fastchunks(av));
 
 
-  /* Request enough space for nb + pad + overhead */
+    /* Request enough space for nb + pad + overhead */
 
-  size = nb + av->top_pad + MINSIZE;
+    size = nb + av->top_pad + MINSIZE;
 
-  /*
-    If contiguous, we can subtract out existing space that we hope to
-    combine with new space. We add it back later only if
-    we don't actually get contiguous space.
-  */
+    /*
+      If contiguous, we can subtract out existing space that we hope to
+      combine with new space. We add it back later only if
+      we don't actually get contiguous space.
+    */
 
-  if (contiguous(av))
-    size -= old_size;
+    if (contiguous(av))
+        size -= old_size;
 
-  /*
-    Round to a multiple of page size.
-    If MORECORE is not contiguous, this ensures that we only call it
-    with whole-page arguments.  And if MORECORE is contiguous and
-    this is not first time through, this preserves page-alignment of
-    previous calls. Otherwise, we correct to page-align below.
-  */
+    /*
+      Round to a multiple of page size.
+      If MORECORE is not contiguous, this ensures that we only call it
+      with whole-page arguments.  And if MORECORE is contiguous and
+      this is not first time through, this preserves page-alignment of
+      previous calls. Otherwise, we correct to page-align below.
+    */
 
-  size = (size + pagemask) & ~pagemask;
+    size = (size + pagemask) & ~pagemask;
 
-  /*
-    Don't try to call MORECORE if argument is so big as to appear
-    negative. Note that since mmap takes size_t arg, it may succeed
-    below even if we cannot call MORECORE.
-  */
+    /*
+      Don't try to call MORECORE if argument is so big as to appear
+      negative. Note that since mmap takes size_t arg, it may succeed
+      below even if we cannot call MORECORE.
+    */
 
-  if (size > 0)
-    brk = (char*)(MORECORE(size));
+    if (size > 0)
+        brk = (char*)(MORECORE(size));
 
-  /*
-    If have mmap, try using it as a backup when MORECORE fails or
-    cannot be used. This is worth doing on systems that have "holes" in
-    address space, so sbrk cannot extend to give contiguous space, but
-    space is available elsewhere.  Note that we ignore mmap max count
-    and threshold limits, since the space will not be used as a
-    segregated mmap region.
-  */
+    /*
+      If have mmap, try using it as a backup when MORECORE fails or
+      cannot be used. This is worth doing on systems that have "holes" in
+      address space, so sbrk cannot extend to give contiguous space, but
+      space is available elsewhere.  Note that we ignore mmap max count
+      and threshold limits, since the space will not be used as a
+      segregated mmap region.
+    */
 
 #if HAVE_MMAP
-  if (brk == (char*)(MORECORE_FAILURE)) {
+    if (brk == (char*)(MORECORE_FAILURE)) {
 
-    /* Cannot merge with old top, so add its size back in */
-    if (contiguous(av))
-      size = (size + old_size + pagemask) & ~pagemask;
+        /* Cannot merge with old top, so add its size back in */
+        if (contiguous(av))
+            size = (size + old_size + pagemask) & ~pagemask;
 
-    /* If we are relying on mmap as backup, then use larger units */
-    if ((CHUNK_SIZE_T)(size) < (CHUNK_SIZE_T)(MMAP_AS_MORECORE_SIZE))
-      size = MMAP_AS_MORECORE_SIZE;
+        /* If we are relying on mmap as backup, then use larger units */
+        if ((CHUNK_SIZE_T)(size) < (CHUNK_SIZE_T)(MMAP_AS_MORECORE_SIZE))
+            size = MMAP_AS_MORECORE_SIZE;
 
-    /* Don't try if size wraps around 0 */
-    if ((CHUNK_SIZE_T)(size) > (CHUNK_SIZE_T)(nb)) {
+        /* Don't try if size wraps around 0 */
+        if ((CHUNK_SIZE_T)(size) > (CHUNK_SIZE_T)(nb)) {
 
-      brk = (char*)(MMAP(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE));
+            brk = (char*)(MMAP(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE));
 
-      if (brk != (char*)(MORECORE_FAILURE)) {
+            if (brk != (char*)(MORECORE_FAILURE)) {
 
-        /* We do not need, and cannot use, another sbrk call to find end */
-        snd_brk = brk + size;
+                /* We do not need, and cannot use, another sbrk call to find end */
+                snd_brk = brk + size;
 
-        /*
-           Record that we no longer have a contiguous sbrk region.
-           After the first time mmap is used as backup, we do not
-           ever rely on contiguous space since this could incorrectly
-           bridge regions.
-        */
-        set_noncontiguous(av);
-      }
+                /*
+                   Record that we no longer have a contiguous sbrk region.
+                   After the first time mmap is used as backup, we do not
+                   ever rely on contiguous space since this could incorrectly
+                   bridge regions.
+                */
+                set_noncontiguous(av);
+            }
+        }
     }
-  }
 #endif
 
-  if (brk != (char*)(MORECORE_FAILURE)) {
-    av->sbrked_mem += size;
+    if (brk != (char*)(MORECORE_FAILURE)) {
+        av->sbrked_mem += size;
 
-    /*
-      If MORECORE extends previous space, we can likewise extend top size.
-    */
+        /*
+          If MORECORE extends previous space, we can likewise extend top size.
+        */
 
-    if (brk == old_end && snd_brk == (char*)(MORECORE_FAILURE)) {
-      set_head(old_top, (size + old_size) | PREV_INUSE);
-    }
+        if (brk == old_end && snd_brk == (char*)(MORECORE_FAILURE)) {
+            set_head(old_top, (size + old_size) | PREV_INUSE);
+        }
 
-    /*
-      Otherwise, make adjustments:
+        /*
+          Otherwise, make adjustments:
 
-      * If the first time through or noncontiguous, we need to call sbrk
-        just to find out where the end of memory lies.
+          * If the first time through or noncontiguous, we need to call sbrk
+            just to find out where the end of memory lies.
 
-      * We need to ensure that all returned chunks from malloc will meet
-        MALLOC_ALIGNMENT
+          * We need to ensure that all returned chunks from malloc will meet
+            MALLOC_ALIGNMENT
 
-      * If there was an intervening foreign sbrk, we need to adjust sbrk
-        request size to account for fact that we will not be able to
-        combine new space with existing space in old_top.
+          * If there was an intervening foreign sbrk, we need to adjust sbrk
+            request size to account for fact that we will not be able to
+            combine new space with existing space in old_top.
 
-      * Almost all systems internally allocate whole pages at a time, in
-        which case we might as well use the whole last page of request.
-        So we allocate enough more memory to hit a page boundary now,
-        which in turn causes future contiguous calls to page-align.
-    */
+          * Almost all systems internally allocate whole pages at a time, in
+            which case we might as well use the whole last page of request.
+            So we allocate enough more memory to hit a page boundary now,
+            which in turn causes future contiguous calls to page-align.
+        */
 
-    else {
-      front_misalign = 0;
-      end_misalign = 0;
-      correction = 0;
-      aligned_brk = brk;
+        else {
+            front_misalign = 0;
+            end_misalign = 0;
+            correction = 0;
+            aligned_brk = brk;
 
-      /*
-        If MORECORE returns an address lower than we have seen before,
-        we know it isn't really contiguous.  This and some subsequent
-        checks help cope with non-conforming MORECORE functions and
-        the presence of "foreign" calls to MORECORE from outside of
-        malloc or by other threads.  We cannot guarantee to detect
-        these in all cases, but cope with the ones we do detect.
-      */
-      if (contiguous(av) && old_size != 0 && brk < old_end) {
-        set_noncontiguous(av);
-      }
+            /*
+              If MORECORE returns an address lower than we have seen before,
+              we know it isn't really contiguous.  This and some subsequent
+              checks help cope with non-conforming MORECORE functions and
+              the presence of "foreign" calls to MORECORE from outside of
+              malloc or by other threads.  We cannot guarantee to detect
+              these in all cases, but cope with the ones we do detect.
+            */
+            if (contiguous(av) && old_size != 0 && brk < old_end) {
+                set_noncontiguous(av);
+            }
 
-      /* handle contiguous cases */
-      if (contiguous(av)) {
+            /* handle contiguous cases */
+            if (contiguous(av)) {
 
-        /*
-           We can tolerate forward non-contiguities here (usually due
-           to foreign calls) but treat them as part of our space for
-           stats reporting.
-        */
-        if (old_size != 0)
-          av->sbrked_mem += brk - old_end;
+                /*
+                   We can tolerate forward non-contiguities here (usually due
+                   to foreign calls) but treat them as part of our space for
+                   stats reporting.
+                */
+                if (old_size != 0)
+                    av->sbrked_mem += brk - old_end;
 
-        /* Guarantee alignment of first new chunk made from this space */
+                /* Guarantee alignment of first new chunk made from this space */
 
-        front_misalign = (INTERNAL_SIZE_T)chunk2mem(brk) & MALLOC_ALIGN_MASK;
-        if (front_misalign > 0) {
+                front_misalign = (INTERNAL_SIZE_T)chunk2mem(brk) & MALLOC_ALIGN_MASK;
+                if (front_misalign > 0) {
 
-          /*
-            Skip over some bytes to arrive at an aligned position.
-            We don't need to specially mark these wasted front bytes.
-            They will never be accessed anyway because
-            prev_inuse of av->top (and any chunk created from its start)
-            is always true after initialization.
-          */
+                    /*
+                      Skip over some bytes to arrive at an aligned position.
+                      We don't need to specially mark these wasted front bytes.
+                      They will never be accessed anyway because
+                      prev_inuse of av->top (and any chunk created from its start)
+                      is always true after initialization.
+                    */
 
-          correction = MALLOC_ALIGNMENT - front_misalign;
-          aligned_brk += correction;
-        }
+                    correction = MALLOC_ALIGNMENT - front_misalign;
+                    aligned_brk += correction;
+                }
 
-        /*
-          If this isn't adjacent to existing space, then we will not
-          be able to merge with old_top space, so must add to 2nd request.
-        */
+                /*
+                  If this isn't adjacent to existing space, then we will not
+                  be able to merge with old_top space, so must add to 2nd request.
+                */
 
-        correction += old_size;
+                correction += old_size;
 
-        /* Extend the end address to hit a page boundary */
-        end_misalign = (INTERNAL_SIZE_T)(brk + size + correction);
-        correction += ((end_misalign + pagemask) & ~pagemask) - end_misalign;
+                /* Extend the end address to hit a page boundary */
+                end_misalign = (INTERNAL_SIZE_T)(brk + size + correction);
+                correction += ((end_misalign + pagemask) & ~pagemask) - end_misalign;
 
-        assert(correction >= 0);
-        snd_brk = (char*)(MORECORE(correction));
+                assert(correction >= 0);
+                snd_brk = (char*)(MORECORE(correction));
 
-        if (snd_brk == (char*)(MORECORE_FAILURE)) {
-          /*
-            If can't allocate correction, try to at least find out current
-            brk.  It might be enough to proceed without failing.
-          */
-          correction = 0;
-          snd_brk = (char*)(MORECORE(0));
-        }
-        else if (snd_brk < brk) {
-          /*
-            If the second call gives noncontiguous space even though
-            it says it won't, the only course of action is to ignore
-            results of second call, and conservatively estimate where
-            the first call left us. Also set noncontiguous, so this
-            won't happen again, leaving at most one hole.
+                if (snd_brk == (char*)(MORECORE_FAILURE)) {
+                    /*
+                      If can't allocate correction, try to at least find out current
+                      brk.  It might be enough to proceed without failing.
+                    */
+                    correction = 0;
+                    snd_brk = (char*)(MORECORE(0));
+                }
+                else if (snd_brk < brk) {
+                    /*
+                      If the second call gives noncontiguous space even though
+                      it says it won't, the only course of action is to ignore
+                      results of second call, and conservatively estimate where
+                      the first call left us. Also set noncontiguous, so this
+                      won't happen again, leaving at most one hole.
 
-            Note that this check is intrinsically incomplete.  Because
-            MORECORE is allowed to give more space than we ask for,
-            there is no reliable way to detect a noncontiguity
-            producing a forward gap for the second call.
-          */
-          snd_brk = brk + size;
-          correction = 0;
-          set_noncontiguous(av);
-        }
+                      Note that this check is intrinsically incomplete.  Because
+                      MORECORE is allowed to give more space than we ask for,
+                      there is no reliable way to detect a noncontiguity
+                      producing a forward gap for the second call.
+                    */
+                    snd_brk = brk + size;
+                    correction = 0;
+                    set_noncontiguous(av);
+                }
+            }
 
-      }
+            /* handle non-contiguous cases */
+            else {
+                /* MORECORE/mmap must correctly align */
+                assert(aligned_OK(chunk2mem(brk)));
 
-      /* handle non-contiguous cases */
-      else {
-        /* MORECORE/mmap must correctly align */
-        assert(aligned_OK(chunk2mem(brk)));
+                /* Find out current end of memory */
+                if (snd_brk == (char*)(MORECORE_FAILURE)) {
+                  snd_brk = (char*)(MORECORE(0));
+                  av->sbrked_mem += snd_brk - brk - size;
+                }
+            }
 
-        /* Find out current end of memory */
-        if (snd_brk == (char*)(MORECORE_FAILURE)) {
-          snd_brk = (char*)(MORECORE(0));
-          av->sbrked_mem += snd_brk - brk - size;
-        }
-      }
+            /* Adjust top based on results of second sbrk */
+            if (snd_brk != (char*)(MORECORE_FAILURE)) {
+                av->top = (mchunkptr)aligned_brk;
+                set_head(av->top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
+                av->sbrked_mem += correction;
 
-      /* Adjust top based on results of second sbrk */
-      if (snd_brk != (char*)(MORECORE_FAILURE)) {
-        av->top = (mchunkptr)aligned_brk;
-        set_head(av->top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
-        av->sbrked_mem += correction;
+                /*
+                  If not the first time through, we either have a
+                  gap due to foreign sbrk or a non-contiguous region.  Insert a
+                  double fencepost at old_top to prevent consolidation with space
+                  we don't own. These fenceposts are artificial chunks that are
+                  marked as inuse and are in any case too small to use.  We need
+                  two to make sizes and alignments work out.
+                */
 
-        /*
-          If not the first time through, we either have a
-          gap due to foreign sbrk or a non-contiguous region.  Insert a
-          double fencepost at old_top to prevent consolidation with space
-          we don't own. These fenceposts are artificial chunks that are
-          marked as inuse and are in any case too small to use.  We need
-          two to make sizes and alignments work out.
-        */
+                if (old_size != 0) {
+                    /*
+                       Shrink old_top to insert fenceposts, keeping size a
+                       multiple of MALLOC_ALIGNMENT. We know there is at least
+                       enough space in old_top to do this.
+                    */
+                    old_size = (old_size - 3*SIZE_SZ) & ~MALLOC_ALIGN_MASK;
+                    set_head(old_top, old_size | PREV_INUSE);
 
-        if (old_size != 0) {
-          /*
-             Shrink old_top to insert fenceposts, keeping size a
-             multiple of MALLOC_ALIGNMENT. We know there is at least
-             enough space in old_top to do this.
-          */
-          old_size = (old_size - 3*SIZE_SZ) & ~MALLOC_ALIGN_MASK;
-          set_head(old_top, old_size | PREV_INUSE);
+                    /*
+                      Note that the following assignments completely overwrite
+                      old_top when old_size was previously MINSIZE.  This is
+                      intentional. We need the fencepost, even if old_top otherwise gets
+                      lost.
+                    */
+                    chunk_at_offset(old_top, old_size)->size =
+                        SIZE_SZ|PREV_INUSE;
 
-          /*
-            Note that the following assignments completely overwrite
-            old_top when old_size was previously MINSIZE.  This is
-            intentional. We need the fencepost, even if old_top otherwise gets
-            lost.
-          */
-          chunk_at_offset(old_top, old_size)->size =
-            SIZE_SZ|PREV_INUSE;
+                    chunk_at_offset(old_top, old_size + SIZE_SZ)->size =
+                        SIZE_SZ|PREV_INUSE;
 
-          chunk_at_offset(old_top, old_size + SIZE_SZ)->size =
-            SIZE_SZ|PREV_INUSE;
-
-          /*
-             If possible, release the rest, suppressing trimming.
-          */
-          if (old_size >= MINSIZE) {
-            INTERNAL_SIZE_T tt = av->trim_threshold;
-            av->trim_threshold = (INTERNAL_SIZE_T)(-1);
-            fREe(chunk2mem(old_top));
-            av->trim_threshold = tt;
-          }
+                    /*
+                       If possible, release the rest, suppressing trimming.
+                    */
+                    if (old_size >= MINSIZE) {
+                        INTERNAL_SIZE_T tt = av->trim_threshold;
+                        av->trim_threshold = (INTERNAL_SIZE_T)(-1);
+                        fREe(chunk2mem(old_top));
+                        av->trim_threshold = tt;
+                    }
+                }
+            }
         }
-      }
-    }
 
-    /* Update statistics */
-    sum = av->sbrked_mem;
-    if (sum > (CHUNK_SIZE_T)(av->max_sbrked_mem))
-      av->max_sbrked_mem = sum;
+        /* Update statistics */
+        sum = av->sbrked_mem;
+        if (sum > (CHUNK_SIZE_T)(av->max_sbrked_mem))
+            av->max_sbrked_mem = sum;
 
-    sum += av->mmapped_mem;
-    if (sum > (CHUNK_SIZE_T)(av->max_total_mem))
-      av->max_total_mem = sum;
+        sum += av->mmapped_mem;
+        if (sum > (CHUNK_SIZE_T)(av->max_total_mem))
+            av->max_total_mem = sum;
 
-    check_malloc_state();
+        check_malloc_state();
 
-    /* finally, do the allocation */
+        /* finally, do the allocation */
 
-    p = av->top;
-    size = chunksize(p);
+        p = av->top;
+        size = chunksize(p);
 
-    /* check that one of the above allocation paths succeeded */
-    if ((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb + MINSIZE)) {
-      remainder_size = size - nb;
-      remainder = chunk_at_offset(p, nb);
-      av->top = remainder;
-      set_head(p, nb | PREV_INUSE);
-      set_head(remainder, remainder_size | PREV_INUSE);
-      check_malloced_chunk(p, nb);
-      return chunk2mem(p);
+        /* check that one of the above allocation paths succeeded */
+        if ((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb + MINSIZE)) {
+            remainder_size = size - nb;
+            remainder = chunk_at_offset(p, nb);
+            av->top = remainder;
+            set_head(p, nb | PREV_INUSE);
+            set_head(remainder, remainder_size | PREV_INUSE);
+            check_malloced_chunk(p, nb);
+            return chunk2mem(p);
+        }
+
     }
 
-  }
-
-  /* catch all failure paths */
-  MALLOC_FAILURE_ACTION;
-  return 0;
+    /* catch all failure paths */
+    MALLOC_FAILURE_ACTION;
+    return 0;
 }
 
 
@@ -3305,55 +3304,55 @@
 static int sYSTRIm(pad, av) size_t pad; mstate av;
 #endif
 {
-  long  top_size;        /* Amount of top-most memory */
-  long  extra;           /* Amount to release */
-  long  released;        /* Amount actually released */
-  char* current_brk;     /* address returned by pre-check sbrk call */
-  char* new_brk;         /* address returned by post-check sbrk call */
-  size_t pagesz;
+    long  top_size;        /* Amount of top-most memory */
+    long  extra;           /* Amount to release */
+    long  released;        /* Amount actually released */
+    char* current_brk;     /* address returned by pre-check sbrk call */
+    char* new_brk;         /* address returned by post-check sbrk call */
+    size_t pagesz;
 
-  pagesz = av->pagesize;
-  top_size = chunksize(av->top);
+    pagesz = av->pagesize;
+    top_size = chunksize(av->top);
 
-  /* Release in pagesize units, keeping at least one page */
-  extra = ((top_size - pad - MINSIZE + (pagesz-1)) / pagesz - 1) * pagesz;
+    /* Release in pagesize units, keeping at least one page */
+    extra = ((top_size - pad - MINSIZE + (pagesz-1)) / pagesz - 1) * pagesz;
 
-  if (extra > 0) {
+    if (extra > 0) {
 
-    /*
-      Only proceed if end of memory is where we last set it.
-      This avoids problems if there were foreign sbrk calls.
-    */
-    current_brk = (char*)(MORECORE(0));
-    if (current_brk == (char*)(av->top) + top_size) {
+        /*
+          Only proceed if end of memory is where we last set it.
+          This avoids problems if there were foreign sbrk calls.
+        */
+        current_brk = (char*)(MORECORE(0));
+        if (current_brk == (char*)(av->top) + top_size) {
 
-      /*
-        Attempt to release memory. We ignore MORECORE return value,
-        and instead call again to find out where new end of memory is.
-        This avoids problems if first call releases less than we asked,
-        of if failure somehow altered brk value. (We could still
-        encounter problems if it altered brk in some very bad way,
-        but the only thing we can do is adjust anyway, which will cause
-        some downstream failure.)
-      */
+            /*
+              Attempt to release memory. We ignore MORECORE return value,
+              and instead call again to find out where new end of memory is.
+              This avoids problems if first call releases less than we asked,
+              of if failure somehow altered brk value. (We could still
+              encounter problems if it altered brk in some very bad way,
+              but the only thing we can do is adjust anyway, which will cause
+              some downstream failure.)
+            */
 
-      MORECORE(-extra);
-      new_brk = (char*)(MORECORE(0));
+            MORECORE(-extra);
+            new_brk = (char*)(MORECORE(0));
 
-      if (new_brk != (char*)MORECORE_FAILURE) {
-        released = (long)(current_brk - new_brk);
+            if (new_brk != (char*)MORECORE_FAILURE) {
+                released = (long)(current_brk - new_brk);
 
-        if (released != 0) {
-          /* Success. Adjust top. */
-          av->sbrked_mem -= released;
-          set_head(av->top, (top_size - released) | PREV_INUSE);
-          check_malloc_state();
-          return 1;
+                if (released != 0) {
+                    /* Success. Adjust top. */
+                    av->sbrked_mem -= released;
+                    set_head(av->top, (top_size - released) | PREV_INUSE);
+                    check_malloc_state();
+                    return 1;
+                }
+            }
         }
-      }
     }
-  }
-  return 0;
+    return 0;
 }
 
 /*
@@ -3364,350 +3363,350 @@
 #if __STD_C
 Void_t* mALLOc(size_t bytes)
 #else
-  Void_t* mALLOc(bytes) size_t bytes;
+    Void_t* mALLOc(bytes) size_t bytes;
 #endif
 {
-  mstate av = get_malloc_state();
+    mstate av = get_malloc_state();
 
-  INTERNAL_SIZE_T nb;               /* normalized request size */
-  unsigned int    idx;              /* associated bin index */
-  mbinptr         bin;              /* associated bin */
-  mfastbinptr*    fb;               /* associated fastbin */
+    INTERNAL_SIZE_T nb;               /* normalized request size */
+    unsigned int    idx;              /* associated bin index */
+    mbinptr         bin;              /* associated bin */
+    mfastbinptr*    fb;               /* associated fastbin */
 
-  mchunkptr       victim;           /* inspected/selected chunk */
-  INTERNAL_SIZE_T size;             /* its size */
-  int             victim_index;     /* its bin index */
+    mchunkptr       victim;           /* inspected/selected chunk */
+    INTERNAL_SIZE_T size;             /* its size */
+    int             victim_index;     /* its bin index */
 
-  mchunkptr       remainder;        /* remainder from a split */
-  CHUNK_SIZE_T    remainder_size;   /* its size */
+    mchunkptr       remainder;        /* remainder from a split */
+    CHUNK_SIZE_T    remainder_size;   /* its size */
 
-  unsigned int    block;            /* bit map traverser */
-  unsigned int    bit;              /* bit map traverser */
-  unsigned int    map;              /* current word of binmap */
+    unsigned int    block;            /* bit map traverser */
+    unsigned int    bit;              /* bit map traverser */
+    unsigned int    map;              /* current word of binmap */
 
-  mchunkptr       fwd;              /* misc temp for linking */
-  mchunkptr       bck;              /* misc temp for linking */
+    mchunkptr       fwd;              /* misc temp for linking */
+    mchunkptr       bck;              /* misc temp for linking */
 
-  /*
-    Convert request size to internal form by adding SIZE_SZ bytes
-    overhead plus possibly more to obtain necessary alignment and/or
-    to obtain a size of at least MINSIZE, the smallest allocatable
-    size. Also, checked_request2size traps (returning 0) request sizes
-    that are so large that they wrap around zero when padded and
-    aligned.
-  */
+    /*
+      Convert request size to internal form by adding SIZE_SZ bytes
+      overhead plus possibly more to obtain necessary alignment and/or
+      to obtain a size of at least MINSIZE, the smallest allocatable
+      size. Also, checked_request2size traps (returning 0) request sizes
+      that are so large that they wrap around zero when padded and
+      aligned.
+    */
 
-  checked_request2size(bytes, nb);
+    checked_request2size(bytes, nb);
 
-  /*
-    Bypass search if no frees yet
-   */
-  if (!have_anychunks(av)) {
-    if (av->max_fast == 0) /* initialization check */
-      malloc_consolidate(av);
-    goto use_top;
-  }
+    /*
+      Bypass search if no frees yet
+     */
+    if (!have_anychunks(av)) {
+        if (av->max_fast == 0) /* initialization check */
+            malloc_consolidate(av);
+        goto use_top;
+    }
 
-  /*
-    If the size qualifies as a fastbin, first check corresponding bin.
-  */
+    /*
+      If the size qualifies as a fastbin, first check corresponding bin.
+    */
 
-  if ((CHUNK_SIZE_T)(nb) <= (CHUNK_SIZE_T)(av->max_fast)) {
-    fb = &(av->fastbins[(fastbin_index(nb))]);
-    if ((victim = *fb) != 0) {
-      *fb = victim->fd;
-      check_remalloced_chunk(victim, nb);
-      return chunk2mem(victim);
+    if ((CHUNK_SIZE_T)(nb) <= (CHUNK_SIZE_T)(av->max_fast)) {
+        fb = &(av->fastbins[(fastbin_index(nb))]);
+        if ((victim = *fb) != 0) {
+            *fb = victim->fd;
+            check_remalloced_chunk(victim, nb);
+            return chunk2mem(victim);
+        }
     }
-  }
 
-  /*
-    If a small request, check regular bin.  Since these "smallbins"
-    hold one size each, no searching within bins is necessary.
-    (For a large request, we need to wait until unsorted chunks are
-    processed to find best fit. But for small ones, fits are exact
-    anyway, so we can check now, which is faster.)
-  */
+    /*
+      If a small request, check regular bin.  Since these "smallbins"
+      hold one size each, no searching within bins is necessary.
+      (For a large request, we need to wait until unsorted chunks are
+      processed to find best fit. But for small ones, fits are exact
+      anyway, so we can check now, which is faster.)
+    */
 
-  if (in_smallbin_range(nb)) {
-    idx = smallbin_index(nb);
-    bin = bin_at(av,idx);
+    if (in_smallbin_range(nb)) {
+        idx = smallbin_index(nb);
+        bin = bin_at(av,idx);
 
-    if ((victim = last(bin)) != bin) {
-      bck = victim->bk;
-      set_inuse_bit_at_offset(victim, nb);
-      bin->bk = bck;
-      bck->fd = bin;
+        if ((victim = last(bin)) != bin) {
+            bck = victim->bk;
+            set_inuse_bit_at_offset(victim, nb);
+            bin->bk = bck;
+            bck->fd = bin;
 
-      check_malloced_chunk(victim, nb);
-      return chunk2mem(victim);
+            check_malloced_chunk(victim, nb);
+            return chunk2mem(victim);
+        }
     }
-  }
 
-  /*
-     If this is a large request, consolidate fastbins before continuing.
-     While it might look excessive to kill all fastbins before
-     even seeing if there is space available, this avoids
-     fragmentation problems normally associated with fastbins.
-     Also, in practice, programs tend to have runs of either small or
-     large requests, but less often mixtures, so consolidation is not
-     invoked all that often in most programs. And the programs that
-     it is called frequently in otherwise tend to fragment.
-  */
+    /*
+       If this is a large request, consolidate fastbins before continuing.
+       While it might look excessive to kill all fastbins before
+       even seeing if there is space available, this avoids
+       fragmentation problems normally associated with fastbins.
+       Also, in practice, programs tend to have runs of either small or
+       large requests, but less often mixtures, so consolidation is not
+       invoked all that often in most programs. And the programs that
+       it is called frequently in otherwise tend to fragment.
+    */
 
-  else {
-    idx = largebin_index(nb);
-    if (have_fastchunks(av))
-      malloc_consolidate(av);
-  }
+    else {
+        idx = largebin_index(nb);
+        if (have_fastchunks(av))
+            malloc_consolidate(av);
+    }
 
-  /*
-    Process recently freed or remaindered chunks, taking one only if
-    it is exact fit, or, if this a small request, the chunk is remainder from
-    the most recent non-exact fit.  Place other traversed chunks in
-    bins.  Note that this step is the only place in any routine where
-    chunks are placed in bins.
-  */
-
-  while ((victim = unsorted_chunks(av)->bk) != unsorted_chunks(av)) {
-    bck = victim->bk;
-    size = chunksize(victim);
-
     /*
-       If a small request, try to use last remainder if it is the
-       only chunk in unsorted bin.  This helps promote locality for
-       runs of consecutive small requests. This is the only
-       exception to best-fit, and applies only when there is
-       no exact fit for a small chunk.
+      Process recently freed or remaindered chunks, taking one only if
+      it is exact fit, or, if this a small request, the chunk is remainder from
+      the most recent non-exact fit.  Place other traversed chunks in
+      bins.  Note that this step is the only place in any routine where
+      chunks are placed in bins.
     */
 
-    if (in_smallbin_range(nb) &&
-        bck == unsorted_chunks(av) &&
-        victim == av->last_remainder &&
-        (CHUNK_SIZE_T)(size) > (CHUNK_SIZE_T)(nb + MINSIZE)) {
+    while ((victim = unsorted_chunks(av)->bk) != unsorted_chunks(av)) {
+        bck = victim->bk;
+        size = chunksize(victim);
 
-      /* split and reattach remainder */
-      remainder_size = size - nb;
-      remainder = chunk_at_offset(victim, nb);
-      unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
-      av->last_remainder = remainder;
-      remainder->bk = remainder->fd = unsorted_chunks(av);
+        /*
+           If a small request, try to use last remainder if it is the
+           only chunk in unsorted bin.  This helps promote locality for
+           runs of consecutive small requests. This is the only
+           exception to best-fit, and applies only when there is
+           no exact fit for a small chunk.
+        */
 
-      set_head(victim, nb | PREV_INUSE);
-      set_head(remainder, remainder_size | PREV_INUSE);
-      set_foot(remainder, remainder_size);
+        if (in_smallbin_range(nb) &&
+            bck == unsorted_chunks(av) &&
+            victim == av->last_remainder &&
+            (CHUNK_SIZE_T)(size) > (CHUNK_SIZE_T)(nb + MINSIZE)) {
 
-      check_malloced_chunk(victim, nb);
-      return chunk2mem(victim);
-    }
+            /* split and reattach remainder */
+            remainder_size = size - nb;
+            remainder = chunk_at_offset(victim, nb);
+            unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
+            av->last_remainder = remainder;
+            remainder->bk = remainder->fd = unsorted_chunks(av);
 
-    /* remove from unsorted list */
-    unsorted_chunks(av)->bk = bck;
-    bck->fd = unsorted_chunks(av);
+            set_head(victim, nb | PREV_INUSE);
+            set_head(remainder, remainder_size | PREV_INUSE);
+            set_foot(remainder, remainder_size);
 
-    /* Take now instead of binning if exact fit */
+            check_malloced_chunk(victim, nb);
+            return chunk2mem(victim);
+        }
 
-    if (size == nb) {
-      set_inuse_bit_at_offset(victim, size);
-      check_malloced_chunk(victim, nb);
-      return chunk2mem(victim);
-    }
+        /* remove from unsorted list */
+        unsorted_chunks(av)->bk = bck;
+        bck->fd = unsorted_chunks(av);
 
-    /* place chunk in bin */
+        /* Take now instead of binning if exact fit */
 
-    if (in_smallbin_range(size)) {
-      victim_index = smallbin_index(size);
-      bck = bin_at(av, victim_index);
-      fwd = bck->fd;
-    }
-    else {
-      victim_index = largebin_index(size);
-      bck = bin_at(av, victim_index);
-      fwd = bck->fd;
+        if (size == nb) {
+            set_inuse_bit_at_offset(victim, size);
+            check_malloced_chunk(victim, nb);
+            return chunk2mem(victim);
+        }
 
-      if (fwd != bck) {
-        /* if smaller than smallest, place first */
-        if ((CHUNK_SIZE_T)(size) < (CHUNK_SIZE_T)(bck->bk->size)) {
-          fwd = bck;
-          bck = bck->bk;
+        /* place chunk in bin */
+
+        if (in_smallbin_range(size)) {
+            victim_index = smallbin_index(size);
+            bck = bin_at(av, victim_index);
+            fwd = bck->fd;
         }
-        else if ((CHUNK_SIZE_T)(size) >=
-                 (CHUNK_SIZE_T)(FIRST_SORTED_BIN_SIZE)) {
+        else {
+            victim_index = largebin_index(size);
+            bck = bin_at(av, victim_index);
+            fwd = bck->fd;
 
-          /* maintain large bins in sorted order */
-          size |= PREV_INUSE; /* Or with inuse bit to speed comparisons */
-          while ((CHUNK_SIZE_T)(size) < (CHUNK_SIZE_T)(fwd->size))
-            fwd = fwd->fd;
-          bck = fwd->bk;
+            if (fwd != bck) {
+                /* if smaller than smallest, place first */
+                if ((CHUNK_SIZE_T)(size) < (CHUNK_SIZE_T)(bck->bk->size)) {
+                    fwd = bck;
+                    bck = bck->bk;
+                }
+                else if ((CHUNK_SIZE_T)(size) >=
+                         (CHUNK_SIZE_T)(FIRST_SORTED_BIN_SIZE)) {
+
+                    /* maintain large bins in sorted order */
+                    size |= PREV_INUSE; /* Or with inuse bit to speed comparisons */
+                    while ((CHUNK_SIZE_T)(size) < (CHUNK_SIZE_T)(fwd->size))
+                      fwd = fwd->fd;
+                    bck = fwd->bk;
+                }
+            }
         }
-      }
+
+        mark_bin(av, victim_index);
+        victim->bk = bck;
+        victim->fd = fwd;
+        fwd->bk = victim;
+        bck->fd = victim;
     }
 
-    mark_bin(av, victim_index);
-    victim->bk = bck;
-    victim->fd = fwd;
-    fwd->bk = victim;
-    bck->fd = victim;
-  }
+    /*
+      If a large request, scan through the chunks of current bin to
+      find one that fits.  (This will be the smallest that fits unless
+      FIRST_SORTED_BIN_SIZE has been changed from default.)  This is
+      the only step where an unbounded number of chunks might be
+      scanned without doing anything useful with them. However the
+      lists tend to be short.
+    */
 
-  /*
-    If a large request, scan through the chunks of current bin to
-    find one that fits.  (This will be the smallest that fits unless
-    FIRST_SORTED_BIN_SIZE has been changed from default.)  This is
-    the only step where an unbounded number of chunks might be
-    scanned without doing anything useful with them. However the
-    lists tend to be short.
-  */
+    if (!in_smallbin_range(nb)) {
+        bin = bin_at(av, idx);
 
-  if (!in_smallbin_range(nb)) {
-    bin = bin_at(av, idx);
+        for (victim = last(bin); victim != bin; victim = victim->bk) {
+            size = chunksize(victim);
 
-    for (victim = last(bin); victim != bin; victim = victim->bk) {
-      size = chunksize(victim);
+            if ((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb)) {
+                remainder_size = size - nb;
+                unlink(victim, bck, fwd);
 
-      if ((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb)) {
-        remainder_size = size - nb;
-        unlink(victim, bck, fwd);
-
-        /* Exhaust */
-        if (remainder_size < MINSIZE)  {
-          set_inuse_bit_at_offset(victim, size);
-          check_malloced_chunk(victim, nb);
-          return chunk2mem(victim);
+                /* Exhaust */
+                if (remainder_size < MINSIZE)  {
+                    set_inuse_bit_at_offset(victim, size);
+                    check_malloced_chunk(victim, nb);
+                    return chunk2mem(victim);
+                }
+                /* Split */
+                else {
+                    remainder = chunk_at_offset(victim, nb);
+                    unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
+                    remainder->bk = remainder->fd = unsorted_chunks(av);
+                    set_head(victim, nb | PREV_INUSE);
+                    set_head(remainder, remainder_size | PREV_INUSE);
+                    set_foot(remainder, remainder_size);
+                    check_malloced_chunk(victim, nb);
+                    return chunk2mem(victim);
+                }
+            }
         }
-        /* Split */
-        else {
-          remainder = chunk_at_offset(victim, nb);
-          unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
-          remainder->bk = remainder->fd = unsorted_chunks(av);
-          set_head(victim, nb | PREV_INUSE);
-          set_head(remainder, remainder_size | PREV_INUSE);
-          set_foot(remainder, remainder_size);
-          check_malloced_chunk(victim, nb);
-          return chunk2mem(victim);
-        }
-      }
     }
-  }
 
-  /*
-    Search for a chunk by scanning bins, starting with next largest
-    bin. This search is strictly by best-fit; i.e., the smallest
-    (with ties going to approximately the least recently used) chunk
-    that fits is selected.
+    /*
+      Search for a chunk by scanning bins, starting with next largest
+      bin. This search is strictly by best-fit; i.e., the smallest
+      (with ties going to approximately the least recently used) chunk
+      that fits is selected.
 
-    The bitmap avoids needing to check that most blocks are nonempty.
-  */
+      The bitmap avoids needing to check that most blocks are nonempty.
+    */
 
-  ++idx;
-  bin = bin_at(av,idx);
-  block = idx2block(idx);
-  map = av->binmap[block];
-  bit = idx2bit(idx);
+    ++idx;
+    bin = bin_at(av,idx);
+    block = idx2block(idx);
+    map = av->binmap[block];
+    bit = idx2bit(idx);
 
-  for (;;) {
+    for (;;) {
 
-    /* Skip rest of block if there are no more set bits in this block.  */
-    if (bit > map || bit == 0) {
-      do {
-        if (++block >= BINMAPSIZE)  /* out of bins */
-          goto use_top;
-      } while ((map = av->binmap[block]) == 0);
+        /* Skip rest of block if there are no more set bits in this block.  */
+        if (bit > map || bit == 0) {
+            do {
+                if (++block >= BINMAPSIZE)  /* out of bins */
+                    goto use_top;
+            } while ((map = av->binmap[block]) == 0);
 
-      bin = bin_at(av, (block << BINMAPSHIFT));
-      bit = 1;
-    }
+            bin = bin_at(av, (block << BINMAPSHIFT));
+            bit = 1;
+        }
 
-    /* Advance to bin with set bit. There must be one. */
-    while ((bit & map) == 0) {
-      bin = next_bin(bin);
-      bit <<= 1;
-      assert(bit != 0);
-    }
+        /* Advance to bin with set bit. There must be one. */
+        while ((bit & map) == 0) {
+            bin = next_bin(bin);
+            bit <<= 1;
+            assert(bit != 0);
+        }
 
-    /* Inspect the bin. It is likely to be non-empty */
-    victim = last(bin);
+        /* Inspect the bin. It is likely to be non-empty */
+        victim = last(bin);
 
-    /*  If a false alarm (empty bin), clear the bit. */
-    if (victim == bin) {
-      av->binmap[block] = map &= ~bit; /* Write through */
-      bin = next_bin(bin);
-      bit <<= 1;
-    }
+        /*  If a false alarm (empty bin), clear the bit. */
+        if (victim == bin) {
+            av->binmap[block] = map &= ~bit; /* Write through */
+            bin = next_bin(bin);
+            bit <<= 1;
+        }
 
-    else {
-      size = chunksize(victim);
+        else {
+            size = chunksize(victim);
 
-      /*  We know the first chunk in this bin is big enough to use. */
-      assert((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb));
+            /*  We know the first chunk in this bin is big enough to use. */
+            assert((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb));
 
-      remainder_size = size - nb;
+            remainder_size = size - nb;
 
-      /* unlink */
-      bck = victim->bk;
-      bin->bk = bck;
-      bck->fd = bin;
+            /* unlink */
+            bck = victim->bk;
+            bin->bk = bck;
+            bck->fd = bin;
 
-      /* Exhaust */
-      if (remainder_size < MINSIZE) {
-        set_inuse_bit_at_offset(victim, size);
-        check_malloced_chunk(victim, nb);
-        return chunk2mem(victim);
-      }
+            /* Exhaust */
+            if (remainder_size < MINSIZE) {
+                set_inuse_bit_at_offset(victim, size);
+                check_malloced_chunk(victim, nb);
+                return chunk2mem(victim);
+            }
 
-      /* Split */
-      else {
-        remainder = chunk_at_offset(victim, nb);
+            /* Split */
+            else {
+                remainder = chunk_at_offset(victim, nb);
 
-        unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
-        remainder->bk = remainder->fd = unsorted_chunks(av);
-        /* advertise as last remainder */
-        if (in_smallbin_range(nb))
-          av->last_remainder = remainder;
+                unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
+                remainder->bk = remainder->fd = unsorted_chunks(av);
+                /* advertise as last remainder */
+                if (in_smallbin_range(nb))
+                    av->last_remainder = remainder;
 
-        set_head(victim, nb | PREV_INUSE);
-        set_head(remainder, remainder_size | PREV_INUSE);
-        set_foot(remainder, remainder_size);
-        check_malloced_chunk(victim, nb);
-        return chunk2mem(victim);
-      }
+                set_head(victim, nb | PREV_INUSE);
+                set_head(remainder, remainder_size | PREV_INUSE);
+                set_foot(remainder, remainder_size);
+                check_malloced_chunk(victim, nb);
+                return chunk2mem(victim);
+            }
+        }
     }
-  }
 
-  use_top:
-  /*
-    If large enough, split off the chunk bordering the end of memory
-    (held in av->top). Note that this is in accord with the best-fit
-    search rule.  In effect, av->top is treated as larger (and thus
-    less well fitting) than any other available chunk since it can
-    be extended to be as large as necessary (up to system
-    limitations).
+    use_top:
+    /*
+      If large enough, split off the chunk bordering the end of memory
+      (held in av->top). Note that this is in accord with the best-fit
+      search rule.  In effect, av->top is treated as larger (and thus
+      less well fitting) than any other available chunk since it can
+      be extended to be as large as necessary (up to system
+      limitations).
 
-    We require that av->top always exists (i.e., has size >=
-    MINSIZE) after initialization, so if it would otherwise be
-    exhausted by the current request, it is replenished. (The main
-    reason for ensuring it exists is that we may need MINSIZE space
-    to put in fenceposts in sysmalloc.)
-  */
+      We require that av->top always exists (i.e., has size >=
+      MINSIZE) after initialization, so if it would otherwise be
+      exhausted by the current request, it is replenished. (The main
+      reason for ensuring it exists is that we may need MINSIZE space
+      to put in fenceposts in sysmalloc.)
+    */
 
-  victim = av->top;
-  size = chunksize(victim);
+    victim = av->top;
+    size = chunksize(victim);
 
-  if ((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb + MINSIZE)) {
-    remainder_size = size - nb;
-    remainder = chunk_at_offset(victim, nb);
-    av->top = remainder;
-    set_head(victim, nb | PREV_INUSE);
-    set_head(remainder, remainder_size | PREV_INUSE);
+    if ((CHUNK_SIZE_T)(size) >= (CHUNK_SIZE_T)(nb + MINSIZE)) {
+        remainder_size = size - nb;
+        remainder = chunk_at_offset(victim, nb);
+        av->top = remainder;
+        set_head(victim, nb | PREV_INUSE);
+        set_head(remainder, remainder_size | PREV_INUSE);
 
-    check_malloced_chunk(victim, nb);
-    return chunk2mem(victim);
-  }
+        check_malloced_chunk(victim, nb);
+        return chunk2mem(victim);
+    }
 
-  /*
-     If no space in top, relay to handle system-dependent cases
-  */
-  return sYSMALLOc(nb, av);
+    /*
+       If no space in top, relay to handle system-dependent cases
+    */
+    return sYSMALLOc(nb, av);
 }
 
 /*
@@ -3720,152 +3719,152 @@
 void fREe(mem) Void_t* mem;
 #endif
 {
-  mstate av = get_malloc_state();
+    mstate av = get_malloc_state();
 
-  mchunkptr       p;           /* chunk corresponding to mem */
-  INTERNAL_SIZE_T size;        /* its size */
-  mfastbinptr*    fb;          /* associated fastbin */
-  mchunkptr       nextchunk;   /* next contiguous chunk */
-  INTERNAL_SIZE_T nextsize;    /* its size */
-  int             nextinuse;   /* true if nextchunk is used */
-  INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
-  mchunkptr       bck;         /* misc temp for linking */
-  mchunkptr       fwd;         /* misc temp for linking */
+    mchunkptr       p;           /* chunk corresponding to mem */
+    INTERNAL_SIZE_T size;        /* its size */
+    mfastbinptr*    fb;          /* associated fastbin */
+    mchunkptr       nextchunk;   /* next contiguous chunk */
+    INTERNAL_SIZE_T nextsize;    /* its size */
+    int             nextinuse;   /* true if nextchunk is used */
+    INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
+    mchunkptr       bck;         /* misc temp for linking */
+    mchunkptr       fwd;         /* misc temp for linking */
 
-  /* free(0) has no effect */
-  if (mem != 0) {
-    p = mem2chunk(mem);
-    size = chunksize(p);
+    /* free(0) has no effect */
+    if (mem != 0) {
+        p = mem2chunk(mem);
+        size = chunksize(p);
 
-    check_inuse_chunk(p);
+        check_inuse_chunk(p);
 
-    /*
-      If eligible, place chunk on a fastbin so it can be found
-      and used quickly in malloc.
-    */
+        /*
+          If eligible, place chunk on a fastbin so it can be found
+          and used quickly in malloc.
+        */
 
-    if ((CHUNK_SIZE_T)(size) <= (CHUNK_SIZE_T)(av->max_fast)
+        if ((CHUNK_SIZE_T)(size) <= (CHUNK_SIZE_T)(av->max_fast)
 
 #if TRIM_FASTBINS
-        /*
-           If TRIM_FASTBINS set, don't place chunks
-           bordering top into fastbins
-        */
-        && (chunk_at_offset(p, size) != av->top)
+            /*
+               If TRIM_FASTBINS set, don't place chunks
+               bordering top into fastbins
+            */
+            && (chunk_at_offset(p, size) != av->top)
 #endif
-        ) {
+            ) {
 
-      set_fastchunks(av);
-      fb = &(av->fastbins[fastbin_index(size)]);
-      p->fd = *fb;
-      *fb = p;
-    }
+            set_fastchunks(av);
+            fb = &(av->fastbins[fastbin_index(size)]);
+            p->fd = *fb;
+            *fb = p;
+        }
 
-    /*
-       Consolidate other non-mmapped chunks as they arrive.
-    */
+        /*
+           Consolidate other non-mmapped chunks as they arrive.
+        */
 
-    else if (!chunk_is_mmapped(p)) {
-      set_anychunks(av);
+        else if (!chunk_is_mmapped(p)) {
+            set_anychunks(av);
 
-      nextchunk = chunk_at_offset(p, size);
-      nextsize = chunksize(nextchunk);
+            nextchunk = chunk_at_offset(p, size);
+            nextsize = chunksize(nextchunk);
 
-      /* consolidate backward */
-      if (!prev_inuse(p)) {
-        prevsize = p->prev_size;
-        size += prevsize;
-        p = chunk_at_offset(p, -((long) prevsize));
-        unlink(p, bck, fwd);
-      }
+            /* consolidate backward */
+            if (!prev_inuse(p)) {
+                prevsize = p->prev_size;
+                size += prevsize;
+                p = chunk_at_offset(p, -((long) prevsize));
+                unlink(p, bck, fwd);
+            }
 
-      if (nextchunk != av->top) {
-        /* get and clear inuse bit */
-        nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
-        set_head(nextchunk, nextsize);
+            if (nextchunk != av->top) {
+                /* get and clear inuse bit */
+                nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
+                set_head(nextchunk, nextsize);
 
-        /* consolidate forward */
-        if (!nextinuse) {
-          unlink(nextchunk, bck, fwd);
-          size += nextsize;
-        }
+                /* consolidate forward */
+                if (!nextinuse) {
+                    unlink(nextchunk, bck, fwd);
+                    size += nextsize;
+                }
 
-        /*
-          Place the chunk in unsorted chunk list. Chunks are
-          not placed into regular bins until after they have
-          been given one chance to be used in malloc.
-        */
+                /*
+                  Place the chunk in unsorted chunk list. Chunks are
+                  not placed into regular bins until after they have
+                  been given one chance to be used in malloc.
+                */
 
-        bck = unsorted_chunks(av);
-        fwd = bck->fd;
-        p->bk = bck;
-        p->fd = fwd;
-        bck->fd = p;
-        fwd->bk = p;
+                bck = unsorted_chunks(av);
+                fwd = bck->fd;
+                p->bk = bck;
+                p->fd = fwd;
+                bck->fd = p;
+                fwd->bk = p;
 
-        set_head(p, size | PREV_INUSE);
-        set_foot(p, size);
+                set_head(p, size | PREV_INUSE);
+                set_foot(p, size);
 
-        check_free_chunk(p);
-      }
+                check_free_chunk(p);
+            }
 
-      /*
-         If the chunk borders the current high end of memory,
-         consolidate into top
-      */
+            /*
+               If the chunk borders the current high end of memory,
+               consolidate into top
+            */
 
-      else {
-        size += nextsize;
-        set_head(p, size | PREV_INUSE);
-        av->top = p;
-        check_chunk(p);
-      }
+            else {
+                size += nextsize;
+                set_head(p, size | PREV_INUSE);
+                av->top = p;
+                check_chunk(p);
+            }
 
-      /*
-        If freeing a large space, consolidate possibly-surrounding
-        chunks. Then, if the total unused topmost memory exceeds trim
-        threshold, ask malloc_trim to reduce top.
+            /*
+              If freeing a large space, consolidate possibly-surrounding
+              chunks. Then, if the total unused topmost memory exceeds trim
+              threshold, ask malloc_trim to reduce top.
 
-        Unless max_fast is 0, we don't know if there are fastbins
-        bordering top, so we cannot tell for sure whether threshold
-        has been reached unless fastbins are consolidated.  But we
-        don't want to consolidate on each free.  As a compromise,
-        consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
-        is reached.
-      */
+              Unless max_fast is 0, we don't know if there are fastbins
+              bordering top, so we cannot tell for sure whether threshold
+              has been reached unless fastbins are consolidated.  But we
+              don't want to consolidate on each free.  As a compromise,
+              consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
+              is reached.
+            */
 
-      if ((CHUNK_SIZE_T)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
-        if (have_fastchunks(av))
-          malloc_consolidate(av);
+            if ((CHUNK_SIZE_T)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
+                if (have_fastchunks(av))
+                    malloc_consolidate(av);
 
 #ifndef MORECORE_CANNOT_TRIM
-        if ((CHUNK_SIZE_T)(chunksize(av->top)) >=
-            (CHUNK_SIZE_T)(av->trim_threshold))
-          sYSTRIm(av->top_pad, av);
+                if ((CHUNK_SIZE_T)(chunksize(av->top)) >=
+                    (CHUNK_SIZE_T)(av->trim_threshold))
+                  sYSTRIm(av->top_pad, av);
 #endif
-      }
+            }
 
-    }
-    /*
-      If the chunk was allocated via mmap, release via munmap()
-      Note that if HAVE_MMAP is false but chunk_is_mmapped is
-      true, then user must have overwritten memory. There's nothing
-      we can do to catch this error unless DEBUG is set, in which case
-      check_inuse_chunk (above) will have triggered error.
-    */
+        }
+        /*
+          If the chunk was allocated via mmap, release via munmap()
+          Note that if HAVE_MMAP is false but chunk_is_mmapped is
+          true, then user must have overwritten memory. There's nothing
+          we can do to catch this error unless DEBUG is set, in which case
+          check_inuse_chunk (above) will have triggered error.
+        */
 
-    else {
+        else {
 #if HAVE_MMAP
-      int ret;
-      INTERNAL_SIZE_T offset = p->prev_size;
-      av->n_mmaps--;
-      av->mmapped_mem -= (size + offset);
-      ret = munmap((char*)p - offset, size + offset);
-      /* munmap returns non-zero on failure */
-      assert(ret == 0);
+            int ret;
+            INTERNAL_SIZE_T offset = p->prev_size;
+            av->n_mmaps--;
+            av->mmapped_mem -= (size + offset);
+            ret = munmap((char*)p - offset, size + offset);
+            /* munmap returns non-zero on failure */
+            assert(ret == 0);
 #endif
+        }
     }
-  }
 }
 
 /*
@@ -3888,96 +3887,96 @@
 static void malloc_consolidate(av) mstate av;
 #endif
 {
-  mfastbinptr*    fb;                 /* current fastbin being consolidated */
-  mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
-  mchunkptr       p;                  /* current chunk being consolidated */
-  mchunkptr       nextp;              /* next chunk to consolidate */
-  mchunkptr       unsorted_bin;       /* bin header */
-  mchunkptr       first_unsorted;     /* chunk to link to */
+    mfastbinptr*    fb;                 /* current fastbin being consolidated */
+    mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
+    mchunkptr       p;                  /* current chunk being consolidated */
+    mchunkptr       nextp;              /* next chunk to consolidate */
+    mchunkptr       unsorted_bin;       /* bin header */
+    mchunkptr       first_unsorted;     /* chunk to link to */
 
-  /* These have same use as in free() */
-  mchunkptr       nextchunk;
-  INTERNAL_SIZE_T size;
-  INTERNAL_SIZE_T nextsize;
-  INTERNAL_SIZE_T prevsize;
-  int             nextinuse;
-  mchunkptr       bck;
-  mchunkptr       fwd;
+    /* These have same use as in free() */
+    mchunkptr       nextchunk;
+    INTERNAL_SIZE_T size;
+    INTERNAL_SIZE_T nextsize;
+    INTERNAL_SIZE_T prevsize;
+    int             nextinuse;
+    mchunkptr       bck;
+    mchunkptr       fwd;
 
-  /*
-    If max_fast is 0, we know that av hasn't
-    yet been initialized, in which case do so below
-  */
+    /*
+      If max_fast is 0, we know that av hasn't
+      yet been initialized, in which case do so below
+    */
 
-  if (av->max_fast != 0) {
-    clear_fastchunks(av);
+    if (av->max_fast != 0) {
+        clear_fastchunks(av);
 
-    unsorted_bin = unsorted_chunks(av);
+        unsorted_bin = unsorted_chunks(av);
 
-    /*
-      Remove each chunk from fast bin and consolidate it, placing it
-      then in unsorted bin. Among other reasons for doing this,
-      placing in unsorted bin avoids needing to calculate actual bins
-      until malloc is sure that chunks aren't immediately going to be
-      reused anyway.
-    */
+        /*
+          Remove each chunk from fast bin and consolidate it, placing it
+          then in unsorted bin. Among other reasons for doing this,
+          placing in unsorted bin avoids needing to calculate actual bins
+          until malloc is sure that chunks aren't immediately going to be
+          reused anyway.
+        */
 
-    maxfb = &(av->fastbins[fastbin_index(av->max_fast)]);
-    fb = &(av->fastbins[0]);
-    do {
-      if ((p = *fb) != 0) {
-        *fb = 0;
-
+        maxfb = &(av->fastbins[fastbin_index(av->max_fast)]);
+        fb = &(av->fastbins[0]);
         do {
-          check_inuse_chunk(p);
-          nextp = p->fd;
+            if ((p = *fb) != 0) {
+                *fb = 0;
 
-          /* Slightly streamlined version of consolidation code in free() */
-          size = p->size & ~PREV_INUSE;
-          nextchunk = chunk_at_offset(p, size);
-          nextsize = chunksize(nextchunk);
+                do {
+                    check_inuse_chunk(p);
+                    nextp = p->fd;
 
-          if (!prev_inuse(p)) {
-            prevsize = p->prev_size;
-            size += prevsize;
-            p = chunk_at_offset(p, -((long) prevsize));
-            unlink(p, bck, fwd);
-          }
+                    /* Slightly streamlined version of consolidation code in free() */
+                    size = p->size & ~PREV_INUSE;
+                    nextchunk = chunk_at_offset(p, size);
+                    nextsize = chunksize(nextchunk);
 
-          if (nextchunk != av->top) {
-            nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
-            set_head(nextchunk, nextsize);
+                    if (!prev_inuse(p)) {
+                        prevsize = p->prev_size;
+                        size += prevsize;
+                        p = chunk_at_offset(p, -((long) prevsize));
+                        unlink(p, bck, fwd);
+                    }
 
-            if (!nextinuse) {
-              size += nextsize;
-              unlink(nextchunk, bck, fwd);
-            }
+                    if (nextchunk != av->top) {
+                        nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
+                        set_head(nextchunk, nextsize);
 
-            first_unsorted = unsorted_bin->fd;
-            unsorted_bin->fd = p;
-            first_unsorted->bk = p;
+                        if (!nextinuse) {
+                            size += nextsize;
+                            unlink(nextchunk, bck, fwd);
+                        }
 
-            set_head(p, size | PREV_INUSE);
-            p->bk = unsorted_bin;
-            p->fd = first_unsorted;
-            set_foot(p, size);
-          }
+                        first_unsorted = unsorted_bin->fd;
+                        unsorted_bin->fd = p;
+                        first_unsorted->bk = p;
 
-          else {
-            size += nextsize;
-            set_head(p, size | PREV_INUSE);
-            av->top = p;
-          }
+                        set_head(p, size | PREV_INUSE);
+                        p->bk = unsorted_bin;
+                        p->fd = first_unsorted;
+                        set_foot(p, size);
+                    }
 
-        } while ((p = nextp) != 0);
+                    else {
+                        size += nextsize;
+                        set_head(p, size | PREV_INUSE);
+                        av->top = p;
+                    }
 
-      }
-    } while (fb++ != maxfb);
-  }
-  else {
-    malloc_init_state(av);
-    check_malloc_state();
-  }
+                } while ((p = nextp) != 0);
+
+            }
+        } while (fb++ != maxfb);
+    }
+    else {
+        malloc_init_state(av);
+        check_malloc_state();
+    }
 }
 
 /*
@@ -3991,220 +3990,221 @@
 Void_t* rEALLOc(oldmem, bytes) Void_t* oldmem; size_t bytes;
 #endif
 {
-  mstate av = get_malloc_state();
+    mstate av = get_malloc_state();
 
-  INTERNAL_SIZE_T  nb;              /* padded request size */
+    INTERNAL_SIZE_T  nb;              /* padded request size */
 
-  mchunkptr        oldp;            /* chunk corresponding to oldmem */
-  INTERNAL_SIZE_T  oldsize;         /* its size */
+    mchunkptr        oldp;            /* chunk corresponding to oldmem */
+    INTERNAL_SIZE_T  oldsize;         /* its size */
 
-  mchunkptr        newp;            /* chunk to return */
-  INTERNAL_SIZE_T  newsize;         /* its size */
-  Void_t*          newmem;          /* corresponding user mem */
+    mchunkptr        newp;            /* chunk to return */
+    INTERNAL_SIZE_T  newsize;         /* its size */
+    Void_t*          newmem;          /* corresponding user mem */
 
-  mchunkptr        next;            /* next contiguous chunk after oldp */
+    mchunkptr        next;            /* next contiguous chunk after oldp */
 
-  mchunkptr        remainder;       /* extra space at end of newp */
-  CHUNK_SIZE_T     remainder_size;  /* its size */
+    mchunkptr        remainder;       /* extra space at end of newp */
+    CHUNK_SIZE_T     remainder_size;  /* its size */
 
-  mchunkptr        bck;             /* misc temp for linking */
-  mchunkptr        fwd;             /* misc temp for linking */
+    mchunkptr        bck;             /* misc temp for linking */
+    mchunkptr        fwd;             /* misc temp for linking */
 
-  CHUNK_SIZE_T     copysize;        /* bytes to copy */
-  unsigned int     ncopies;         /* INTERNAL_SIZE_T words to copy */
-  INTERNAL_SIZE_T* s;               /* copy source */
-  INTERNAL_SIZE_T* d;               /* copy destination */
+    CHUNK_SIZE_T     copysize;        /* bytes to copy */
+    unsigned int     ncopies;         /* INTERNAL_SIZE_T words to copy */
+    INTERNAL_SIZE_T* s;               /* copy source */
+    INTERNAL_SIZE_T* d;               /* copy destination */
 
 
 #ifdef REALLOC_ZERO_BYTES_FREES
-  if (bytes == 0) {
-    fREe(oldmem);
-    return 0;
-  }
+    if (bytes == 0) {
+        fREe(oldmem);
+        return 0;
+    }
 #endif
 
-  /* realloc of null is supposed to be same as malloc */
-  if (oldmem == 0) return mALLOc(bytes);
+    /* realloc of null is supposed to be same as malloc */
+    if (oldmem == 0)
+        return mALLOc(bytes);
 
-  checked_request2size(bytes, nb);
+    checked_request2size(bytes, nb);
 
-  oldp    = mem2chunk(oldmem);
-  oldsize = chunksize(oldp);
+    oldp    = mem2chunk(oldmem);
+    oldsize = chunksize(oldp);
 
-  check_inuse_chunk(oldp);
+    check_inuse_chunk(oldp);
 
-  if (!chunk_is_mmapped(oldp)) {
+    if (!chunk_is_mmapped(oldp)) {
 
-    if ((CHUNK_SIZE_T)(oldsize) >= (CHUNK_SIZE_T)(nb)) {
-      /* already big enough; split below */
-      newp = oldp;
-      newsize = oldsize;
-    }
+        if ((CHUNK_SIZE_T)(oldsize) >= (CHUNK_SIZE_T)(nb)) {
+            /* already big enough; split below */
+            newp = oldp;
+            newsize = oldsize;
+        }
 
-    else {
-      next = chunk_at_offset(oldp, oldsize);
+        else {
+            next = chunk_at_offset(oldp, oldsize);
 
-      /* Try to expand forward into top */
-      if (next == av->top &&
-          (CHUNK_SIZE_T)(newsize = oldsize + chunksize(next)) >=
-          (CHUNK_SIZE_T)(nb + MINSIZE)) {
-        set_head_size(oldp, nb);
-        av->top = chunk_at_offset(oldp, nb);
-        set_head(av->top, (newsize - nb) | PREV_INUSE);
-        return chunk2mem(oldp);
-      }
+            /* Try to expand forward into top */
+            if (next == av->top &&
+                (CHUNK_SIZE_T)(newsize = oldsize + chunksize(next)) >=
+                (CHUNK_SIZE_T)(nb + MINSIZE)) {
+                set_head_size(oldp, nb);
+                av->top = chunk_at_offset(oldp, nb);
+                set_head(av->top, (newsize - nb) | PREV_INUSE);
+                return chunk2mem(oldp);
+            }
 
-      /* Try to expand forward into next chunk;  split off remainder below */
-      else if (next != av->top &&
-               !inuse(next) &&
-               (CHUNK_SIZE_T)(newsize = oldsize + chunksize(next)) >=
-               (CHUNK_SIZE_T)(nb)) {
-        newp = oldp;
-        unlink(next, bck, fwd);
-      }
+            /* Try to expand forward into next chunk;  split off remainder below */
+            else if (next != av->top &&
+                     !inuse(next) &&
+                     (CHUNK_SIZE_T)(newsize = oldsize + chunksize(next)) >=
+                     (CHUNK_SIZE_T)(nb)) {
+                newp = oldp;
+                unlink(next, bck, fwd);
+            }
 
-      /* allocate, copy, free */
-      else {
-        newmem = mALLOc(nb - MALLOC_ALIGN_MASK);
-        if (newmem == 0)
-          return 0; /* propagate failure */
+            /* allocate, copy, free */
+            else {
+                newmem = mALLOc(nb - MALLOC_ALIGN_MASK);
+                if (newmem == 0)
+                    return 0; /* propagate failure */
 
-        newp = mem2chunk(newmem);
-        newsize = chunksize(newp);
+                newp = mem2chunk(newmem);
+                newsize = chunksize(newp);
 
-        /*
-          Avoid copy if newp is next chunk after oldp.
-        */
-        if (newp == next) {
-          newsize += oldsize;
-          newp = oldp;
-        }
-        else {
-          /*
-            Unroll copy of <= 36 bytes (72 if 8byte sizes)
-            We know that contents have an odd number of
-            INTERNAL_SIZE_T-sized words; minimally 3.
-          */
+                /*
+                  Avoid copy if newp is next chunk after oldp.
+                */
+                if (newp == next) {
+                    newsize += oldsize;
+                    newp = oldp;
+                }
+                else {
+                    /*
+                      Unroll copy of <= 36 bytes (72 if 8byte sizes)
+                      We know that contents have an odd number of
+                      INTERNAL_SIZE_T-sized words; minimally 3.
+                    */
 
-          copysize = oldsize - SIZE_SZ;
-          s = (INTERNAL_SIZE_T*)(oldmem);
-          d = (INTERNAL_SIZE_T*)(newmem);
-          ncopies = copysize / sizeof (INTERNAL_SIZE_T);
-          assert(ncopies >= 3);
+                    copysize = oldsize - SIZE_SZ;
+                    s = (INTERNAL_SIZE_T*)(oldmem);
+                    d = (INTERNAL_SIZE_T*)(newmem);
+                    ncopies = copysize / sizeof (INTERNAL_SIZE_T);
+                    assert(ncopies >= 3);
 
-          if (ncopies > 9)
-            MALLOC_COPY(d, s, copysize);
+                    if (ncopies > 9)
+                        MALLOC_COPY(d, s, copysize);
 
-          else {
-            *(d+0) = *(s+0);
-            *(d+1) = *(s+1);
-            *(d+2) = *(s+2);
-            if (ncopies > 4) {
-              *(d+3) = *(s+3);
-              *(d+4) = *(s+4);
-              if (ncopies > 6) {
-                *(d+5) = *(s+5);
-                *(d+6) = *(s+6);
-                if (ncopies > 8) {
-                  *(d+7) = *(s+7);
-                  *(d+8) = *(s+8);
+                    else {
+                        *(d+0) = *(s+0);
+                        *(d+1) = *(s+1);
+                        *(d+2) = *(s+2);
+                        if (ncopies > 4) {
+                            *(d+3) = *(s+3);
+                            *(d+4) = *(s+4);
+                            if (ncopies > 6) {
+                                *(d+5) = *(s+5);
+                                *(d+6) = *(s+6);
+                                if (ncopies > 8) {
+                                    *(d+7) = *(s+7);
+                                    *(d+8) = *(s+8);
+                                }
+                            }
+                        }
+                    }
+
+                    fREe(oldmem);
+                    check_inuse_chunk(newp);
+                    return chunk2mem(newp);
                 }
-              }
             }
-          }
-
-          fREe(oldmem);
-          check_inuse_chunk(newp);
-          return chunk2mem(newp);
         }
-      }
-    }
 
-    /* If possible, free extra space in old or extended chunk */
+        /* If possible, free extra space in old or extended chunk */
 
-    assert((CHUNK_SIZE_T)(newsize) >= (CHUNK_SIZE_T)(nb));
+        assert((CHUNK_SIZE_T)(newsize) >= (CHUNK_SIZE_T)(nb));
 
-    remainder_size = newsize - nb;
+        remainder_size = newsize - nb;
 
-    if (remainder_size < MINSIZE) { /* not enough extra to split off */
-      set_head_size(newp, newsize);
-      set_inuse_bit_at_offset(newp, newsize);
+        if (remainder_size < MINSIZE) { /* not enough extra to split off */
+            set_head_size(newp, newsize);
+            set_inuse_bit_at_offset(newp, newsize);
+        }
+        else { /* split remainder */
+            remainder = chunk_at_offset(newp, nb);
+            set_head_size(newp, nb);
+            set_head(remainder, remainder_size | PREV_INUSE);
+            /* Mark remainder as inuse so free() won't complain */
+            set_inuse_bit_at_offset(remainder, remainder_size);
+            fREe(chunk2mem(remainder));
+        }
+
+        check_inuse_chunk(newp);
+        return chunk2mem(newp);
     }
-    else { /* split remainder */
-      remainder = chunk_at_offset(newp, nb);
-      set_head_size(newp, nb);
-      set_head(remainder, remainder_size | PREV_INUSE);
-      /* Mark remainder as inuse so free() won't complain */
-      set_inuse_bit_at_offset(remainder, remainder_size);
-      fREe(chunk2mem(remainder));
-    }
 
-    check_inuse_chunk(newp);
-    return chunk2mem(newp);
-  }
+    /*
+      Handle mmap cases
+    */
 
-  /*
-    Handle mmap cases
-  */
-
-  else {
+    else {
 #if HAVE_MMAP
 
 #  if HAVE_MREMAP
-    INTERNAL_SIZE_T offset = oldp->prev_size;
-    size_t pagemask = av->pagesize - 1;
-    char *cp;
-    CHUNK_SIZE_T  sum;
+        INTERNAL_SIZE_T offset = oldp->prev_size;
+        size_t pagemask = av->pagesize - 1;
+        char *cp;
+        CHUNK_SIZE_T  sum;
 
-    /* Note the extra SIZE_SZ overhead */
-    newsize = (nb + offset + SIZE_SZ + pagemask) & ~pagemask;
+        /* Note the extra SIZE_SZ overhead */
+        newsize = (nb + offset + SIZE_SZ + pagemask) & ~pagemask;
 
-    /* don't need to remap if still within same page */
-    if (oldsize == newsize - offset)
-      return oldmem;
+        /* don't need to remap if still within same page */
+        if (oldsize == newsize - offset)
+            return oldmem;
 
-    cp = (char*)mremap((char*)oldp - offset, oldsize + offset, newsize, 1);
+        cp = (char*)mremap((char*)oldp - offset, oldsize + offset, newsize, 1);
 
-    if (cp != (char*)MORECORE_FAILURE) {
+        if (cp != (char*)MORECORE_FAILURE) {
 
-      newp = (mchunkptr)(cp + offset);
-      set_head(newp, (newsize - offset)|IS_MMAPPED);
+            newp = (mchunkptr)(cp + offset);
+            set_head(newp, (newsize - offset)|IS_MMAPPED);
 
-      assert(aligned_OK(chunk2mem(newp)));
-      assert((newp->prev_size == offset));
+            assert(aligned_OK(chunk2mem(newp)));
+            assert((newp->prev_size == offset));
 
-      /* update statistics */
-      sum = av->mmapped_mem += newsize - oldsize;
-      if (sum > (CHUNK_SIZE_T)(av->max_mmapped_mem))
-        av->max_mmapped_mem = sum;
-      sum += av->sbrked_mem;
-      if (sum > (CHUNK_SIZE_T)(av->max_total_mem))
-        av->max_total_mem = sum;
+            /* update statistics */
+            sum = av->mmapped_mem += newsize - oldsize;
+            if (sum > (CHUNK_SIZE_T)(av->max_mmapped_mem))
+                av->max_mmapped_mem = sum;
+            sum += av->sbrked_mem;
+            if (sum > (CHUNK_SIZE_T)(av->max_total_mem))
+                av->max_total_mem = sum;
 
-      return chunk2mem(newp);
-    }
+            return chunk2mem(newp);
+        }
 #  endif
 
-    /* Note the extra SIZE_SZ overhead. */
-    if ((CHUNK_SIZE_T)(oldsize) >= (CHUNK_SIZE_T)(nb + SIZE_SZ))
-      newmem = oldmem; /* do nothing */
-    else {
-      /* Must alloc, copy, free. */
-      newmem = mALLOc(nb - MALLOC_ALIGN_MASK);
-      if (newmem != 0) {
-        MALLOC_COPY(newmem, oldmem, oldsize - 2*SIZE_SZ);
-        fREe(oldmem);
-      }
-    }
-    return newmem;
+        /* Note the extra SIZE_SZ overhead. */
+        if ((CHUNK_SIZE_T)(oldsize) >= (CHUNK_SIZE_T)(nb + SIZE_SZ))
+            newmem = oldmem; /* do nothing */
+        else {
+            /* Must alloc, copy, free. */
+            newmem = mALLOc(nb - MALLOC_ALIGN_MASK);
+            if (newmem != 0) {
+                MALLOC_COPY(newmem, oldmem, oldsize - 2*SIZE_SZ);
+                fREe(oldmem);
+            }
+        }
+        return newmem;
 
 #else
-    /* If !HAVE_MMAP, but chunk_is_mmapped, user must have overwritten mem */
-    check_malloc_state();
-    MALLOC_FAILURE_ACTION;
-    return 0;
+        /* If !HAVE_MMAP, but chunk_is_mmapped, user must have overwritten mem */
+        check_malloc_state();
+        MALLOC_FAILURE_ACTION;
+        return 0;
 #endif
-  }
+    }
 }
 
 /*
@@ -4217,99 +4217,103 @@
 Void_t* mEMALIGn(alignment, bytes) size_t alignment; size_t bytes;
 #endif
 {
-  INTERNAL_SIZE_T nb;             /* padded  request size */
-  char*           m;              /* memory returned by malloc call */
-  mchunkptr       p;              /* corresponding chunk */
-  char*           brk;            /* alignment point within p */
-  mchunkptr       newp;           /* chunk to return */
-  INTERNAL_SIZE_T newsize;        /* its size */
-  INTERNAL_SIZE_T leadsize;       /* leading space before alignment point */
-  mchunkptr       remainder;      /* spare room at end to split off */
-  CHUNK_SIZE_T    remainder_size; /* its size */
-  INTERNAL_SIZE_T size;
+    INTERNAL_SIZE_T nb;             /* padded  request size */
+    char*           m;              /* memory returned by malloc call */
+    mchunkptr       p;              /* corresponding chunk */
+    char*           brk;            /* alignment point within p */
+    mchunkptr       newp;           /* chunk to return */
+    INTERNAL_SIZE_T newsize;        /* its size */
+    INTERNAL_SIZE_T leadsize;       /* leading space before alignment point */
+    mchunkptr       remainder;      /* spare room at end to split off */
+    CHUNK_SIZE_T    remainder_size; /* its size */
+    INTERNAL_SIZE_T size;
 
-  /* If need less alignment than we give anyway, just relay to malloc */
+    /* If need less alignment than we give anyway, just relay to malloc */
 
-  if (alignment <= MALLOC_ALIGNMENT) return mALLOc(bytes);
+    if (alignment <= MALLOC_ALIGNMENT)
+        return mALLOc(bytes);
 
-  /* Otherwise, ensure that it is at least a minimum chunk size */
+    /* Otherwise, ensure that it is at least a minimum chunk size */
 
-  if (alignment <  MINSIZE) alignment = MINSIZE;
+    if (alignment <  MINSIZE)
+        alignment = MINSIZE;
 
-  /* Make sure alignment is power of 2 (in case MINSIZE is not).  */
-  if ((alignment & (alignment - 1)) != 0) {
-    size_t a = MALLOC_ALIGNMENT * 2;
-    while ((CHUNK_SIZE_T)a < (CHUNK_SIZE_T)alignment) a <<= 1;
-    alignment = a;
-  }
+    /* Make sure alignment is power of 2 (in case MINSIZE is not).  */
+    if ((alignment & (alignment - 1)) != 0) {
+        size_t a = MALLOC_ALIGNMENT * 2;
+        while ((CHUNK_SIZE_T)a < (CHUNK_SIZE_T)alignment)
+            a <<= 1;
+        alignment = a;
+    }
 
-  checked_request2size(bytes, nb);
+    checked_request2size(bytes, nb);
 
-  /*
-    Strategy: find a spot within that chunk that meets the alignment
-    request, and then possibly free the leading and trailing space.
-  */
+    /*
+      Strategy: find a spot within that chunk that meets the alignment
+      request, and then possibly free the leading and trailing space.
+    */
 
 
-  /* Call malloc with worst case padding to hit alignment. */
+    /* Call malloc with worst case padding to hit alignment. */
 
-  m  = (char*)(mALLOc(nb + alignment + MINSIZE));
+    m  = (char*)(mALLOc(nb + alignment + MINSIZE));
 
-  if (m == 0) return 0; /* propagate failure */
+    if (m == 0)
+        return 0; /* propagate failure */
 
-  p = mem2chunk(m);
+    p = mem2chunk(m);
 
-  if ((((PTR_UINT)(m)) % alignment) != 0) { /* misaligned */
+    if ((((PTR_UINT)(m)) % alignment) != 0) { /* misaligned */
 
-    /*
-      Find an aligned spot inside chunk.  Since we need to give back
-      leading space in a chunk of at least MINSIZE, if the first
-      calculation places us at a spot with less than MINSIZE leader,
-      we can move to the next aligned spot -- we've allocated enough
-      total room so that this is always possible.
-    */
+        /*
+          Find an aligned spot inside chunk.  Since we need to give back
+          leading space in a chunk of at least MINSIZE, if the first
+          calculation places us at a spot with less than MINSIZE leader,
+          we can move to the next aligned spot -- we've allocated enough
+          total room so that this is always possible.
+        */
 
-    brk = (char*)mem2chunk((PTR_UINT)(((PTR_UINT)(m + alignment - 1)) &
-                           -((signed long) alignment)));
-    if ((CHUNK_SIZE_T)(brk - (char*)(p)) < MINSIZE)
-      brk += alignment;
+        brk = (char*)mem2chunk((PTR_UINT)(((PTR_UINT)(m + alignment - 1)) &
+                               -((signed long) alignment)));
+        if ((CHUNK_SIZE_T)(brk - (char*)(p)) < MINSIZE)
+            brk += alignment;
 
-    newp = (mchunkptr)brk;
-    leadsize = brk - (char*)(p);
-    newsize = chunksize(p) - leadsize;
+        newp = (mchunkptr)brk;
+        leadsize = brk - (char*)(p);
+        newsize = chunksize(p) - leadsize;
 
-    /* For mmapped chunks, just adjust offset */
-    if (chunk_is_mmapped(p)) {
-      newp->prev_size = p->prev_size + leadsize;
-      set_head(newp, newsize|IS_MMAPPED);
-      return chunk2mem(newp);
-    }
+        /* For mmapped chunks, just adjust offset */
+        if (chunk_is_mmapped(p)) {
+            newp->prev_size = p->prev_size + leadsize;
+            set_head(newp, newsize|IS_MMAPPED);
+            return chunk2mem(newp);
+        }
 
-    /* Otherwise, give back leader, use the rest */
-    set_head(newp, newsize | PREV_INUSE);
-    set_inuse_bit_at_offset(newp, newsize);
-    set_head_size(p, leadsize);
-    fREe(chunk2mem(p));
-    p = newp;
+        /* Otherwise, give back leader, use the rest */
+        set_head(newp, newsize | PREV_INUSE);
+        set_inuse_bit_at_offset(newp, newsize);
+        set_head_size(p, leadsize);
+        fREe(chunk2mem(p));
+        p = newp;
 
-    assert (newsize >= nb &&
-            (((PTR_UINT)(chunk2mem(p))) % alignment) == 0);
-  }
+        assert (newsize >= nb &&
+                (((PTR_UINT)(chunk2mem(p))) % alignment) == 0);
+    }
 
-  /* Also give back spare room at the end */
-  if (!chunk_is_mmapped(p)) {
-    size = chunksize(p);
-    if ((CHUNK_SIZE_T)(size) > (CHUNK_SIZE_T)(nb + MINSIZE)) {
-      remainder_size = size - nb;
-      remainder = chunk_at_offset(p, nb);
-      set_head(remainder, remainder_size | PREV_INUSE);
-      set_head_size(p, nb);
-      fREe(chunk2mem(remainder));
+    /* Also give back spare room at the end */
+    if (!chunk_is_mmapped(p)) {
+        size = chunksize(p);
+        if ((CHUNK_SIZE_T)(size) > (CHUNK_SIZE_T)(nb + MINSIZE)) {
+            remainder_size = size - nb;
+            remainder = chunk_at_offset(p, nb);
+            set_head(remainder, remainder_size | PREV_INUSE);
+            set_head_size(p, nb);
+            fREe(chunk2mem(remainder));
+        }
     }
-  }
 
-  check_inuse_chunk(p);
-  return chunk2mem(p);
+    check_inuse_chunk(p);
+    return chunk2mem(p);
 }
 
 /*
@@ -4322,63 +4326,63 @@
 Void_t* cALLOc(n_elements, elem_size) size_t n_elements; size_t elem_size;
 #endif
 {
-  mchunkptr p;
-  CHUNK_SIZE_T  clearsize;
-  CHUNK_SIZE_T  nclears;
-  INTERNAL_SIZE_T* d;
+    mchunkptr p;
+    CHUNK_SIZE_T  clearsize;
+    CHUNK_SIZE_T  nclears;
+    INTERNAL_SIZE_T* d;
 
-  Void_t* mem = mALLOc(n_elements * elem_size);
+    Void_t* mem = mALLOc(n_elements * elem_size);
 
-  if (mem != 0) {
-    p = mem2chunk(mem);
+    if (mem != 0) {
+        p = mem2chunk(mem);
 
-    if (!chunk_is_mmapped(p))
-    {
-      /*
-        Unroll clear of <= 36 bytes (72 if 8byte sizes)
-        We know that contents have an odd number of
-        INTERNAL_SIZE_T-sized words; minimally 3.
-      */
+        if (!chunk_is_mmapped(p))
+        {
+            /*
+              Unroll clear of <= 36 bytes (72 if 8byte sizes)
+              We know that contents have an odd number of
+              INTERNAL_SIZE_T-sized words; minimally 3.
+            */
 
-      d = (INTERNAL_SIZE_T*)mem;
-      clearsize = chunksize(p) - SIZE_SZ;
-      nclears = clearsize / sizeof (INTERNAL_SIZE_T);
-      assert(nclears >= 3);
+            d = (INTERNAL_SIZE_T*)mem;
+            clearsize = chunksize(p) - SIZE_SZ;
+            nclears = clearsize / sizeof (INTERNAL_SIZE_T);
+            assert(nclears >= 3);
 
-      if (nclears > 9)
-        MALLOC_ZERO(d, clearsize);
+            if (nclears > 9)
+                MALLOC_ZERO(d, clearsize);
 
-      else {
-        *(d+0) = 0;
-        *(d+1) = 0;
-        *(d+2) = 0;
-        if (nclears > 4) {
-          *(d+3) = 0;
-          *(d+4) = 0;
-          if (nclears > 6) {
-            *(d+5) = 0;
-            *(d+6) = 0;
-            if (nclears > 8) {
-              *(d+7) = 0;
-              *(d+8) = 0;
+            else {
+                *(d+0) = 0;
+                *(d+1) = 0;
+                *(d+2) = 0;
+                if (nclears > 4) {
+                    *(d+3) = 0;
+                    *(d+4) = 0;
+                    if (nclears > 6) {
+                        *(d+5) = 0;
+                        *(d+6) = 0;
+                        if (nclears > 8) {
+                            *(d+7) = 0;
+                            *(d+8) = 0;
+                        }
+                    }
+                }
             }
-          }
         }
-      }
-    }
 #if ! MMAP_CLEARS
-    else
-    {
-      d = (INTERNAL_SIZE_T*)mem;
-      /*
-        Note the additional SIZE_SZ
-      */
-      clearsize = chunksize(p) - 2*SIZE_SZ;
-      MALLOC_ZERO(d, clearsize);
+        else
+        {
+            d = (INTERNAL_SIZE_T*)mem;
+            /*
+              Note the additional SIZE_SZ
+            */
+            clearsize = chunksize(p) - 2*SIZE_SZ;
+            MALLOC_ZERO(d, clearsize);
+        }
+#endif
     }
-#endif
-  }
-  return mem;
+    return mem;
 }
 
 /*
@@ -4391,7 +4395,7 @@
 void cFREe(mem) Void_t *mem;
 #endif
 {
-  fREe(mem);
+    fREe(mem);
 }
 
 /*
@@ -4406,9 +4410,9 @@
                                                 Void_t* chunks[];
 #endif
 {
-  size_t sz = elem_size; /* serves as 1-element array */
-  /* opts arg of 3 means all elements are same size, and should be cleared */
-  return iALLOc(n_elements, &sz, 3, chunks);
+    size_t sz = elem_size; /* serves as 1-element array */
+    /* opts arg of 3 means all elements are same size, and should be cleared */
+    return iALLOc(n_elements, &sz, 3, chunks);
 }
 
 /*
@@ -4418,12 +4422,13 @@
 #if __STD_C
 Void_t** iCOMALLOc(size_t n_elements, size_t sizes[], Void_t* chunks[])
 #else
-Void_t** iCOMALLOc(n_elements, sizes, chunks) size_t n_elements;
-                                              size_t sizes[];
+Void_t** iCOMALLOc(n_elements, sizes, chunks)
+size_t n_elements;
+size_t sizes[];
 Void_t* chunks[];
 #endif
 {
-  return iALLOc(n_elements, sizes, 0, chunks);
+    return iALLOc(n_elements, sizes, 0, chunks);
 }
 
 
@@ -4450,114 +4455,115 @@
                                                         Void_t* chunks[];
 #endif
 {
-  mstate av = get_malloc_state();
-  INTERNAL_SIZE_T element_size;   /* chunksize of each element, if all same */
-  INTERNAL_SIZE_T contents_size;  /* total size of elements */
-  INTERNAL_SIZE_T array_size;     /* request size of pointer array */
-  Void_t*         mem;            /* malloced aggregate space */
-  mchunkptr       p;              /* corresponding chunk */
-  INTERNAL_SIZE_T remainder_size; /* remaining bytes while splitting */
-  Void_t**        marray;         /* either "chunks" or malloced ptr array */
-  mchunkptr       array_chunk;    /* chunk for malloced ptr array */
-  int             mmx;            /* to disable mmap */
-  INTERNAL_SIZE_T size;
-  size_t          i;
+    mstate av = get_malloc_state();
+    INTERNAL_SIZE_T element_size;   /* chunksize of each element, if all same */
+    INTERNAL_SIZE_T contents_size;  /* total size of elements */
+    INTERNAL_SIZE_T array_size;     /* request size of pointer array */
+    Void_t*         mem;            /* malloced aggregate space */
+    mchunkptr       p;              /* corresponding chunk */
+    INTERNAL_SIZE_T remainder_size; /* remaining bytes while splitting */
+    Void_t**        marray;         /* either "chunks" or malloced ptr array */
+    mchunkptr       array_chunk;    /* chunk for malloced ptr array */
+    int             mmx;            /* to disable mmap */
+    INTERNAL_SIZE_T size;
+    size_t          i;
 
-  /* Ensure initialization */
-  if (av->max_fast == 0) malloc_consolidate(av);
+    /* Ensure initialization */
+    if (av->max_fast == 0)
+        malloc_consolidate(av);
 
-  /* compute array length, if needed */
-  if (chunks != 0) {
-    if (n_elements == 0)
-      return chunks; /* nothing to do */
-    marray = chunks;
-    array_size = 0;
-  }
-  else {
-    /* if empty req, must still return chunk representing empty array */
-    if (n_elements == 0)
-      return (Void_t**) mALLOc(0);
-    marray = 0;
-    array_size = request2size(n_elements * (sizeof (Void_t*)));
-  }
+    /* compute array length, if needed */
+    if (chunks != 0) {
+        if (n_elements == 0)
+            return chunks; /* nothing to do */
+        marray = chunks;
+        array_size = 0;
+    }
+    else {
+        /* if empty req, must still return chunk representing empty array */
+        if (n_elements == 0)
+            return (Void_t**) mALLOc(0);
+        marray = 0;
+        array_size = request2size(n_elements * (sizeof (Void_t*)));
+    }
 
-  /* compute total element size */
-  if (opts & 0x1) { /* all-same-size */
-    element_size = request2size(*sizes);
-    contents_size = n_elements * element_size;
-  }
-  else { /* add up all the sizes */
-    element_size = 0;
-    contents_size = 0;
-    for (i = 0; i != n_elements; ++i)
-      contents_size += request2size(sizes[i]);
-  }
+    /* compute total element size */
+    if (opts & 0x1) { /* all-same-size */
+        element_size = request2size(*sizes);
+        contents_size = n_elements * element_size;
+    }
+    else { /* add up all the sizes */
+        element_size = 0;
+        contents_size = 0;
+        for (i = 0; i != n_elements; ++i)
+            contents_size += request2size(sizes[i]);
+    }
 
-  /* subtract out alignment bytes from total to minimize overallocation */
-  size = contents_size + array_size - MALLOC_ALIGN_MASK;
+    /* subtract out alignment bytes from total to minimize overallocation */
+    size = contents_size + array_size - MALLOC_ALIGN_MASK;
 
-  /*
-     Allocate the aggregate chunk.
-     But first disable mmap so malloc won't use it, since
-     we would not be able to later free/realloc space internal
-     to a segregated mmap region.
- */
-  mmx = av->n_mmaps_max;   /* disable mmap */
-  av->n_mmaps_max = 0;
-  mem = mALLOc(size);
-  av->n_mmaps_max = mmx;   /* reset mmap */
-  if (mem == 0)
-    return 0;
+    /*
+       Allocate the aggregate chunk.
+       But first disable mmap so malloc won't use it, since
+       we would not be able to later free/realloc space internal
+       to a segregated mmap region.
+   */
+    mmx = av->n_mmaps_max;   /* disable mmap */
+    av->n_mmaps_max = 0;
+    mem = mALLOc(size);
+    av->n_mmaps_max = mmx;   /* reset mmap */
+    if (mem == 0)
+        return 0;
 
-  p = mem2chunk(mem);
-  assert(!chunk_is_mmapped(p));
-  remainder_size = chunksize(p);
+    p = mem2chunk(mem);
+    assert(!chunk_is_mmapped(p));
+    remainder_size = chunksize(p);
 
-  if (opts & 0x2) {       /* optionally clear the elements */
-    MALLOC_ZERO(mem, remainder_size - SIZE_SZ - array_size);
-  }
+    if (opts & 0x2) {       /* optionally clear the elements */
+        MALLOC_ZERO(mem, remainder_size - SIZE_SZ - array_size);
+    }
 
-  /* If not provided, allocate the pointer array as final part of chunk */
-  if (marray == 0) {
-    array_chunk = chunk_at_offset(p, contents_size);
-    marray = (Void_t**) (chunk2mem(array_chunk));
-    set_head(array_chunk, (remainder_size - contents_size) | PREV_INUSE);
-    remainder_size = contents_size;
-  }
+    /* If not provided, allocate the pointer array as final part of chunk */
+    if (marray == 0) {
+        array_chunk = chunk_at_offset(p, contents_size);
+        marray = (Void_t**) (chunk2mem(array_chunk));
+        set_head(array_chunk, (remainder_size - contents_size) | PREV_INUSE);
+        remainder_size = contents_size;
+    }
 
-  /* split out elements */
-  for (i = 0; ; ++i) {
-    marray[i] = chunk2mem(p);
-    if (i != n_elements-1) {
-      if (element_size != 0)
-        size = element_size;
-      else
-        size = request2size(sizes[i]);
-      remainder_size -= size;
-      set_head(p, size | PREV_INUSE);
-      p = chunk_at_offset(p, size);
+    /* split out elements */
+    for (i = 0; ; ++i) {
+        marray[i] = chunk2mem(p);
+        if (i != n_elements-1) {
+            if (element_size != 0)
+                size = element_size;
+            else
+                size = request2size(sizes[i]);
+            remainder_size -= size;
+            set_head(p, size | PREV_INUSE);
+            p = chunk_at_offset(p, size);
+        }
+        else { /* the final element absorbs any overallocation slop */
+            set_head(p, remainder_size | PREV_INUSE);
+            break;
+        }
     }
-    else { /* the final element absorbs any overallocation slop */
-      set_head(p, remainder_size | PREV_INUSE);
-      break;
-    }
-  }
 
 #if DEBUG
-  if (marray != chunks) {
-    /* final element must have exactly exhausted chunk */
-    if (element_size != 0)
-      assert(remainder_size == element_size);
-    else
-      assert(remainder_size == request2size(sizes[i]));
-    check_inuse_chunk(mem2chunk(marray));
-  }
+    if (marray != chunks) {
+        /* final element must have exactly exhausted chunk */
+        if (element_size != 0)
+            assert(remainder_size == element_size);
+        else
+            assert(remainder_size == request2size(sizes[i]));
+        check_inuse_chunk(mem2chunk(marray));
+    }
 
-  for (i = 0; i != n_elements; ++i)
-    check_inuse_chunk(mem2chunk(marray[i]));
+    for (i = 0; i != n_elements; ++i)
+        check_inuse_chunk(mem2chunk(marray[i]));
 #endif
 
-  return marray;
+    return marray;
 }
 
 
@@ -4571,10 +4577,11 @@
 Void_t* vALLOc(bytes) size_t bytes;
 #endif
 {
-  /* Ensure initialization */
-  mstate av = get_malloc_state();
-  if (av->max_fast == 0) malloc_consolidate(av);
-  return mEMALIGn(av->pagesize, bytes);
+    /* Ensure initialization */
+    mstate av = get_malloc_state();
+    if (av->max_fast == 0)
+        malloc_consolidate(av);
+    return mEMALIGn(av->pagesize, bytes);
 }
 
 /*
@@ -4588,13 +4595,14 @@
 Void_t* pVALLOc(bytes) size_t bytes;
 #endif
 {
-  mstate av = get_malloc_state();
-  size_t pagesz;
+    mstate av = get_malloc_state();
+    size_t pagesz;
 
-  /* Ensure initialization */
-  if (av->max_fast == 0) malloc_consolidate(av);
-  pagesz = av->pagesize;
-  return mEMALIGn(pagesz, (bytes + pagesz - 1) & ~(pagesz - 1));
+    /* Ensure initialization */
+    if (av->max_fast == 0)
+        malloc_consolidate(av);
+    pagesz = av->pagesize;
+    return mEMALIGn(pagesz, (bytes + pagesz - 1) & ~(pagesz - 1));
 }
 
 
@@ -4608,14 +4616,14 @@
 int mTRIm(pad) size_t pad;
 #endif
 {
-  mstate av = get_malloc_state();
-  /* Ensure initialization/consolidation */
-  malloc_consolidate(av);
+    mstate av = get_malloc_state();
+    /* Ensure initialization/consolidation */
+    malloc_consolidate(av);
 
 #ifndef MORECORE_CANNOT_TRIM
-  return sYSTRIm(pad, av);
+    return sYSTRIm(pad, av);
 #else
-  return 0;
+    return 0;
 #endif
 }
 
@@ -4630,15 +4638,15 @@
 size_t mUSABLe(mem) Void_t* mem;
 #endif
 {
-  mchunkptr p;
-  if (mem != 0) {
-    p = mem2chunk(mem);
-    if (chunk_is_mmapped(p))
-      return chunksize(p) - 2*SIZE_SZ;
-    else if (inuse(p))
-      return chunksize(p) - SIZE_SZ;
-  }
-  return 0;
+    mchunkptr p;
+    if (mem != 0) {
+        p = mem2chunk(mem);
+        if (chunk_is_mmapped(p))
+            return chunksize(p) - 2*SIZE_SZ;
+        else if (inuse(p))
+            return chunksize(p) - SIZE_SZ;
+    }
+    return 0;
 }
 
 /*
@@ -4647,58 +4655,59 @@
 
 struct mallinfo mALLINFo()
 {
-  mstate av = get_malloc_state();
-  struct mallinfo mi;
-  int i;
-  mbinptr b;
-  mchunkptr p;
-  INTERNAL_SIZE_T avail;
-  INTERNAL_SIZE_T fastavail;
-  int nblocks;
-  int nfastblocks;
+    mstate av = get_malloc_state();
+    struct mallinfo mi;
+    int i;
+    mbinptr b;
+    mchunkptr p;
+    INTERNAL_SIZE_T avail;
+    INTERNAL_SIZE_T fastavail;
+    int nblocks;
+    int nfastblocks;
 
-  /* Ensure initialization */
-  if (av->top == 0)  malloc_consolidate(av);
+    /* Ensure initialization */
+    if (av->top == 0)
+        malloc_consolidate(av);
 
-  check_malloc_state();
+    check_malloc_state();
 
-  /* Account for top */
-  avail = chunksize(av->top);
-  nblocks = 1;  /* top always exists */
+    /* Account for top */
+    avail = chunksize(av->top);
+    nblocks = 1;  /* top always exists */
 
-  /* traverse fastbins */
-  nfastblocks = 0;
-  fastavail = 0;
+    /* traverse fastbins */
+    nfastblocks = 0;
+    fastavail = 0;
 
-  for (i = 0; i < NFASTBINS; ++i) {
-    for (p = av->fastbins[i]; p != 0; p = p->fd) {
-      ++nfastblocks;
-      fastavail += chunksize(p);
+    for (i = 0; i < NFASTBINS; ++i) {
+        for (p = av->fastbins[i]; p != 0; p = p->fd) {
+            ++nfastblocks;
+            fastavail += chunksize(p);
+        }
     }
-  }
 
-  avail += fastavail;
+    avail += fastavail;
 
-  /* traverse regular bins */
-  for (i = 1; i < NBINS; ++i) {
-    b = bin_at(av, i);
-    for (p = last(b); p != b; p = p->bk) {
-      ++nblocks;
-      avail += chunksize(p);
+    /* traverse regular bins */
+    for (i = 1; i < NBINS; ++i) {
+        b = bin_at(av, i);
+        for (p = last(b); p != b; p = p->bk) {
+            ++nblocks;
+            avail += chunksize(p);
+        }
     }
-  }
 
-  mi.smblks = nfastblocks;
-  mi.ordblks = nblocks;
-  mi.fordblks = avail;
-  mi.uordblks = av->sbrked_mem - avail;
-  mi.arena = av->sbrked_mem;
-  mi.hblks = av->n_mmaps;
-  mi.hblkhd = av->mmapped_mem;
-  mi.fsmblks = fastavail;
-  mi.keepcost = chunksize(av->top);
-  mi.usmblks = av->max_total_mem;
-  return mi;
+    mi.smblks = nfastblocks;
+    mi.ordblks = nblocks;
+    mi.fordblks = avail;
+    mi.uordblks = av->sbrked_mem - avail;
+    mi.arena = av->sbrked_mem;
+    mi.hblks = av->n_mmaps;
+    mi.hblkhd = av->mmapped_mem;
+    mi.fsmblks = fastavail;
+    mi.keepcost = chunksize(av->top);
+    mi.usmblks = av->max_total_mem;
+    return mi;
 }
 
 /*
@@ -4707,39 +4716,39 @@
 
 void mSTATs()
 {
-  struct mallinfo mi = mALLINFo();
+    struct mallinfo mi = mALLINFo();
 
 #ifdef WIN32
-  {
-    CHUNK_SIZE_T  free, reserved, committed;
-    vminfo (&free, &reserved, &committed);
-    PIO_eprintf(NULL, "free bytes       = %10lu\n",
-            free);
-    PIO_eprintf(NULL, "reserved bytes   = %10lu\n",
-            reserved);
-    PIO_eprintf(NULL, "committed bytes  = %10lu\n",
-            committed);
-  }
+    {
+        CHUNK_SIZE_T  free, reserved, committed;
+        vminfo (&free, &reserved, &committed);
+        PIO_eprintf(NULL, "free bytes       = %10lu\n",
+                free);
+        PIO_eprintf(NULL, "reserved bytes   = %10lu\n",
+                reserved);
+        PIO_eprintf(NULL, "committed bytes  = %10lu\n",
+                committed);
+    }
 #endif
 
 
-  PIO_eprintf(NULL, "max system bytes = %10lu\n",
-          (CHUNK_SIZE_T)(mi.usmblks));
-  PIO_eprintf(NULL, "system bytes     = %10lu\n",
-          (CHUNK_SIZE_T)(mi.arena + mi.hblkhd));
-  PIO_eprintf(NULL, "in use bytes     = %10lu\n",
-          (CHUNK_SIZE_T)(mi.uordblks + mi.hblkhd));
+    PIO_eprintf(NULL, "max system bytes = %10lu\n",
+            (CHUNK_SIZE_T)(mi.usmblks));
+    PIO_eprintf(NULL, "system bytes     = %10lu\n",
+            (CHUNK_SIZE_T)(mi.arena + mi.hblkhd));
+    PIO_eprintf(NULL, "in use bytes     = %10lu\n",
+            (CHUNK_SIZE_T)(mi.uordblks + mi.hblkhd));
 
 #ifdef WIN32
-  {
-    CHUNK_SIZE_T  kernel, user;
-    if (cpuinfo (TRUE, &kernel, &user)) {
-      PIO_eprintf(NULL, "kernel ms        = %10lu\n",
-              kernel);
-      PIO_eprintf(NULL, "user ms          = %10lu\n",
-              user);
+    {
+        CHUNK_SIZE_T  kernel, user;
+        if (cpuinfo (TRUE, &kernel, &user)) {
+            PIO_eprintf(NULL, "kernel ms        = %10lu\n",
+                    kernel);
+            PIO_eprintf(NULL, "user ms          = %10lu\n",
+                    user);
+        }
     }
-  }
 #endif
 }
 
@@ -4754,42 +4763,42 @@
 int mALLOPt(param_number, value) int param_number; int value;
 #endif
 {
-  mstate av = get_malloc_state();
-  /* Ensure initialization/consolidation */
-  malloc_consolidate(av);
+    mstate av = get_malloc_state();
+    /* Ensure initialization/consolidation */
+    malloc_consolidate(av);
 
-  switch (param_number) {
-  case M_MXFAST:
-    if (value >= 0 && value <= MAX_FAST_SIZE) {
-      set_max_fast(av, value);
-      return 1;
-    }
-    else
-      return 0;
+    switch (param_number) {
+    case M_MXFAST:
+      if (value >= 0 && value <= MAX_FAST_SIZE) {
+          set_max_fast(av, value);
+          return 1;
+      }
+      else
+          return 0;
 
-  case M_TRIM_THRESHOLD:
-    av->trim_threshold = value;
-    return 1;
+    case M_TRIM_THRESHOLD:
+        av->trim_threshold = value;
+        return 1;
 
-  case M_TOP_PAD:
-    av->top_pad = value;
-    return 1;
+    case M_TOP_PAD:
+        av->top_pad = value;
+        return 1;
 
-  case M_MMAP_THRESHOLD:
-    av->mmap_threshold = value;
-    return 1;
+    case M_MMAP_THRESHOLD:
+        av->mmap_threshold = value;
+        return 1;
 
-  case M_MMAP_MAX:
+    case M_MMAP_MAX:
 #if !HAVE_MMAP
-    if (value != 0)
-      return 0;
+        if (value != 0)
+            return 0;
 #endif
-    av->n_mmaps_max = value;
-    return 1;
+        av->n_mmaps_max = value;
+        return 1;
 
-  default:
-    return 0;
-  }
+    default:
+        return 0;
+    }
 }
 
 
@@ -4889,35 +4898,35 @@
 
   void *osMoreCore(int size)
   {
-    void *ptr = 0;
-    static void *sbrk_top = 0;
+      void *ptr = 0;
+      static void *sbrk_top = 0;
 
-    if (size > 0)
-    {
-      if (size < MINIMUM_MORECORE_SIZE)
-         size = MINIMUM_MORECORE_SIZE;
-      if (CurrentExecutionLevel() == kTaskLevel)
-         ptr = PoolAllocateResident(size + RM_PAGE_SIZE, 0);
-      if (ptr == 0)
+      if (size > 0)
       {
-        return (void *) MORECORE_FAILURE;
+          if (size < MINIMUM_MORECORE_SIZE)
+               size = MINIMUM_MORECORE_SIZE;
+          if (CurrentExecutionLevel() == kTaskLevel)
+               ptr = PoolAllocateResident(size + RM_PAGE_SIZE, 0);
+          if (ptr == 0)
+          {
+              return (void *) MORECORE_FAILURE;
+          }
+          / / save ptrs so they can be freed during cleanup
+          our_os_pools[next_os_pool] = ptr;
+          next_os_pool++;
+          ptr = (void *) ((((CHUNK_SIZE_T) ptr) + RM_PAGE_MASK) & ~RM_PAGE_MASK);
+          sbrk_top = (char *) ptr + size;
+          return ptr;
       }
-      / / save ptrs so they can be freed during cleanup
-      our_os_pools[next_os_pool] = ptr;
-      next_os_pool++;
-      ptr = (void *) ((((CHUNK_SIZE_T) ptr) + RM_PAGE_MASK) & ~RM_PAGE_MASK);
-      sbrk_top = (char *) ptr + size;
-      return ptr;
-    }
-    else if (size < 0)
-    {
-      / / we don't currently support shrink behavior
-      return (void *) MORECORE_FAILURE;
-    }
-    else
-    {
-      return sbrk_top;
-    }
+      else if (size < 0)
+      {
+          / / we don't currently support shrink behavior
+          return (void *) MORECORE_FAILURE;
+      }
+      else
+      {
+          return sbrk_top;
+      }
   }
 
   / / cleanup any allocated memory pools
@@ -4925,14 +4934,14 @@
 
   void osCleanupMem(void)
   {
-    void **ptr;
+      void **ptr;
 
-    for (ptr = our_os_pools; ptr < &our_os_pools[MAX_POOL_ENTRIES]; ptr++)
-      if (*ptr)
-      {
-         PoolDeallocate(*ptr);
-         *ptr = 0;
-      }
+      for (ptr = our_os_pools; ptr < &our_os_pools[MAX_POOL_ENTRIES]; ptr++)
+          if (*ptr)
+          {
+               PoolDeallocate(*ptr);
+               *ptr = 0;
+          }
   }
 
 */
Index: src/jit/ppc/jit_emit.h
===================================================================
--- src/jit/ppc/jit_emit.h	(revision 18348)
+++ src/jit/ppc/jit_emit.h	(working copy)
@@ -845,7 +845,7 @@
     Parrot_jit_normal_op(jit_info, interpreter);
     /* test return value; if zero (e.g after trace), return from JIT */
     jit_emit_cmp_ri(jit_info->native_ptr, r3, 0);
-     /* remember PC */
+    /* remember PC */
     jmp_ptr = jit_info->native_ptr;
     /* emit jump past exit code, dummy offset */
     _emit_bc(jit_info->native_ptr, BNE, 0, 0, 0);
Index: src/jit/sun4/jit_emit.h
===================================================================
--- src/jit/sun4/jit_emit.h	(revision 18348)
+++ src/jit/sun4/jit_emit.h	(working copy)
@@ -902,7 +902,7 @@
                 emitm_or_i(jit_info->native_ptr, XSR1, emitm_lo10(SC_addr), XSR1);
 
                 emitm_ld_i(jit_info->native_ptr, XSR1, 0, emitm_o(rdx));
-              break;
+                break;
             case PARROT_ARG_KC:
             case PARROT_ARG_PC:
 #    define KC_addr &interp->code->const_table->constants[pi]->u.key
@@ -910,7 +910,7 @@
                 emitm_or_i(jit_info->native_ptr, XSR1, emitm_lo10(KC_addr), XSR1);
 
                 emitm_ld_i(jit_info->native_ptr, XSR1, 0, emitm_o(rdx));
-              break;
+                break;
             default:
                 internal_exception(1,
                         "jit_vtable_n_op: unimp type %d, arg %d vtable %d",
Index: src/mmd.c
===================================================================
--- src/mmd.c	(revision 18348)
+++ src/mmd.c	(working copy)
@@ -151,8 +151,8 @@
         return func;
     }
     else if (!is_pmc_ptr(interp, F2DPTR(func_))) {
-      *is_pmc = 0;
-      return func;
+        *is_pmc = 0;
+        return func;
     }
 #endif
     return func_;
Index: config/gen/platform/netbsd/math.c
===================================================================
--- config/gen/platform/netbsd/math.c	(revision 18348)
+++ config/gen/platform/netbsd/math.c	(working copy)
@@ -23,15 +23,15 @@
 PARROT_API extern int
 Parrot_signbit(double x)
 {
-   union {
-       double d;
-       int i[2];
-   } u;
-   u.d = x;
+    union {
+        double d;
+        int i[2];
+    } u;
+    u.d = x;
 #  if PARROT_BIGENDIAN
-   return u.i[0] < 0;
+    return u.i[0] < 0;
 #  else
-   return u.i[1] < 0;
+    return u.i[1] < 0;
 #  endif
 }
 #endif
@@ -40,12 +40,12 @@
 int
 Parrot_signbit_l(long double x)
 {
-   union {
-       long double d;
-       int i[3];
-   } u;
-   u.d = x;
-   return u.i[2] < 0;
+    union {
+        long double d;
+        int i[3];
+    } u;
+    u.d = x;
+    return u.i[2] < 0;
 }
 #endif
 
Index: config/gen/platform/cygwin/math.c
===================================================================
--- config/gen/platform/cygwin/math.c	(revision 18348)
+++ config/gen/platform/cygwin/math.c	(working copy)
@@ -21,15 +21,15 @@
 PARROT_API extern int
 Parrot_signbit(double x)
 {
-   union {
-       double d;
-       int i[2];
-   } u;
-   u.d = x;
+    union {
+        double d;
+        int i[2];
+    } u;
+    u.d = x;
 #  if PARROT_BIGENDIAN
-   return u.i[0] < 0;
+    return u.i[0] < 0;
 #  else
-   return u.i[1] < 0;
+    return u.i[1] < 0;
 #  endif
 }
 #endif
@@ -38,12 +38,12 @@
 int
 Parrot_signbit_l(long double x)
 {
-   union {
-       long double d;
-       int i[3];
-   } u;
-   u.d = x;
-   return u.i[2] < 0;
+    union {
+        long double d;
+        int i[3];
+    } u;
+    u.d = x;
+    return u.i[2] < 0;
 }
 #endif
 
Index: config/gen/platform/openbsd/math.c
===================================================================
--- config/gen/platform/openbsd/math.c	(revision 18348)
+++ config/gen/platform/openbsd/math.c	(working copy)
@@ -21,15 +21,15 @@
 PARROT_API extern int
 Parrot_signbit(double x)
 {
-   union {
-       double d;
-       int i[2];
-   } u;
-   u.d = x;
+    union {
+        double d;
+        int i[2];
+    } u;
+    u.d = x;
 #  if PARROT_BIGENDIAN
-   return u.i[0] < 0;
+    return u.i[0] < 0;
 #  else
-   return u.i[1] < 0;
+    return u.i[1] < 0;
 #  endif
 }
 #endif
@@ -38,12 +38,12 @@
 int
 Parrot_signbit_l(long double x)
 {
-   union {
-       long double d;
-       int i[3];
-   } u;
-   u.d = x;
-   return u.i[2] < 0;
+    union {
+        long double d;
+        int i[3];
+    } u;
+    u.d = x;
+    return u.i[2] < 0;
 }
 #endif
 
Index: config/gen/platform/generic/math.c
===================================================================
--- config/gen/platform/generic/math.c	(revision 18348)
+++ config/gen/platform/generic/math.c	(working copy)
@@ -13,15 +13,15 @@
 PARROT_API extern int
 Parrot_signbit(double x)
 {
-   union {
-       double d;
-       int i[2];
-   } u;
-   u.d = x;
+    union {
+        double d;
+        int i[2];
+    } u;
+    u.d = x;
 #  if PARROT_BIGENDIAN
-   return u.i[0] < 0;
+    return u.i[0] < 0;
 #  else
-   return u.i[1] < 0;
+    return u.i[1] < 0;
 #  endif
 }
 #endif
@@ -30,12 +30,12 @@
 int
 Parrot_signbit_l(long double x)
 {
-   union {
-       long double d;
-       int i[3];
-   } u;
-   u.d = x;
-   return u.i[2] < 0;
+    union {
+        long double d;
+        int i[3];
+    } u;
+    u.d = x;
+    return u.i[2] < 0;
 }
 #endif
 
Index: config/gen/platform/solaris/math.c
===================================================================
--- config/gen/platform/solaris/math.c	(revision 18348)
+++ config/gen/platform/solaris/math.c	(working copy)
@@ -22,15 +22,15 @@
 PARROT_API extern int
 Parrot_signbit(double x)
 {
-   union {
-       double d;
-       int i[2];
-   } u;
-   u.d = x;
+    union {
+        double d;
+        int i[2];
+    } u;
+    u.d = x;
 #  if PARROT_BIGENDIAN
-   return u.i[0] < 0;
+    return u.i[0] < 0;
 #  else
-   return u.i[1] < 0;
+    return u.i[1] < 0;
 #  endif
 }
 #endif
@@ -39,12 +39,12 @@
 int
 Parrot_signbit_l(long double x)
 {
-   union {
-       long double d;
-       int i[3];
-   } u;
-   u.d = x;
-   return u.i[2] < 0;
+    union {
+        long double d;
+        int i[3];
+    } u;
+    u.d = x;
+    return u.i[2] < 0;
 }
 #endif
 

------------=_1177754072-30201-220--

