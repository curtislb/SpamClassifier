From r-help-bounces@stat.math.ethz.ch  Tue Jun 12 19:25:27 2007
Return-Path: <r-help-bounces@stat.math.ethz.ch>
Received: from hypatia.math.ethz.ch (hypatia.math.ethz.ch [129.132.145.15])
	by flax9.uwaterloo.ca (8.12.8/8.12.5) with ESMTP id l5CNPQL9000663
	for <ktwarwic@flax9.uwaterloo.ca>; Tue, 12 Jun 2007 19:25:26 -0400
Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l5CNOrjM008160;
	Wed, 13 Jun 2007 01:25:05 +0200
X-Spam-Checker-Version: SpamAssassin 3.2.0 (2007-05-01) on hypatia.math.ethz.ch
X-Spam-Level: *
X-Spam-Status: No, score=1.9 required=5.0 tests=AWL, BAYES_50,
	DKIM_POLICY_SIGNSOME, RCVD_NUMERIC_HELO,
	SARE_MILLIONSOF autolearn=no version=3.2.0
Received: from heisenberg.zen.co.uk (heisenberg.zen.co.uk [212.23.3.141])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l5CNOCw7007927
	for <r-help@stat.math.ethz.ch>; Wed, 13 Jun 2007 01:24:13 +0200
Received: from [88.96.44.206] (helo=88.96.44.206)
	by heisenberg.zen.co.uk with esmtp (Exim 4.50)
	id 1HyFie-0002nB-DW; Tue, 12 Jun 2007 23:24:12 +0000
Received: (from efh@localhost) by 88.96.44.206 (8.8.8/8.8.5) id AAA24711;
	Wed, 13 Jun 2007 00:24:10 +0100
Message-ID: <XFMail.070613002410.ted.harding@nessie.mcc.ac.uk>
X-Mailer: XFMail 1.3-alpha-031298 [p0] on Linux
X-Priority: 3 (Normal)
MIME-Version: 1.0
In-Reply-To: <11083540.post@talk.nabble.com>
Date: Wed, 13 Jun 2007 00:24:10 +0100 (BST)
From: (Ted Harding) <ted.harding@nessie.mcc.ac.uk>
To: r-help@stat.math.ethz.ch
X-Originating-Heisenberg-IP: [88.96.44.206]
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
Subject: Re: [R] Appropriate regression model for categorical variables
X-BeenThere: r-help@stat.math.ethz.ch
X-Mailman-Version: 2.1.9
Precedence: list
Reply-To: ted.harding@manchester.ac.uk
List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=unsubscribe>
List-Archive: <https://stat.ethz.ch/pipermail/r-help>
List-Post: <mailto:r-help@stat.math.ethz.ch>
List-Help: <mailto:r-help-request@stat.math.ethz.ch?subject=help>
List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Sender: r-help-bounces@stat.math.ethz.ch
Errors-To: r-help-bounces@stat.math.ethz.ch

On 12-Jun-07 17:45:44, Tirthadeep wrote:
> 
> Dear users,
> In my psychometric test i have applied logistic regression
> on my data. My data consists of 50 predictors (22 continuous
> and 28 categorical) plus a binary response. 
> 
> Using glm(), stepAIC() i didn't get satisfactory result as
> misclassification rate is too high. I think categorical
> variables are responsible for this debacle. Some of them have
> more than 6 level (one has 10 level).
> 
> Please suggest some better regression model for this situation.
> If possible you can suggest some article.

I hope you have a very large number of cases in your data!

The minimal complexity of the 28 categorical variables compatible
with your description is

  1 factor at 10 levels
  2 factors at 7 levels
 25 factors at 2 levels

which corresponds to (2^25)*(7^2)*10 = 16441671680 ~= 1.6e10
distinct possible combinations of levels of the factors. Your
true factors may have far more than this.

Unless you have more cases than this in your data, you are
likely to fall into what is called "linear separation", in which
the logistic regression will find a perfect predictor for your
binary outcome. This prefect predictor may well not be unique
(indeed if you have only a few hundred cases there will probably
be millions of them).

Therefore your logistic reggression is likely to be meaningless.

I can only suggest that you consider very closely how to

a) reduce the numbers of levels in some of your factors,
   by coalescing levels together;
b) defining new factors in terms of the old so as to reduce
   the total number of factors (which may include ignoring
   some factors altogether)

so that you end up with new categorical variables whose total
number of possible combinations is much smaller (say at most 1/5)
of the number of cases in your data.

In summary: you have to many explanatory variables.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding@manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jun-07                                       Time: 00:23:49
------------------------------ XFMail ------------------------------

______________________________________________
R-help@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

