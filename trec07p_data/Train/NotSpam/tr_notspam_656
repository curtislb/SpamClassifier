From r-help-bounces@stat.math.ethz.ch  Wed Apr 11 02:11:35 2007
Return-Path: <r-help-bounces@stat.math.ethz.ch>
Received: from hypatia.math.ethz.ch (hypatia.math.ethz.ch [129.132.145.15])
	by speedy.uwaterloo.ca (8.12.8/8.12.5) with ESMTP id l3B6BX0I013398
	for <ktwarwic@speedy.uwaterloo.ca>; Wed, 11 Apr 2007 02:11:34 -0400
Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l3B6A3pw000713;
	Wed, 11 Apr 2007 08:10:17 +0200
X-Spam-Checker-Version: SpamAssassin 3.1.8 (2007-02-13) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, score=-0.4 required=5.0 tests=AWL,
	BAYES_50 autolearn=no version=3.1.8
Received: from webd2.ihc.com (webd2.ihc.com [199.190.170.10])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l3AMS76I005597
	for <r-help@stat.math.ethz.ch>; Wed, 11 Apr 2007 00:28:08 +0200
Received: from twemf1.co.ihc.com ([159.212.71.180])
	by webd2.ihc.com with esmtp (Exim 4.62)
	(envelope-from <Greg.Snow@intermountainmail.org>)
	id 1HbOom-0003gI-T3; Tue, 10 Apr 2007 16:28:05 -0600
Received: from 159.212.71.187 by twemf1.co.ihc.com with ESMTP (Email
	Firewall SMTP Relay (Email Firewall v6.2.1)); Tue, 10 Apr 2007 16:27:17
	-0600
X-Server-Uuid: 0CCC7504-CEE2-47B5-B80E-0BE86FBE3DA5
Received: from lp-exchfe02.co.ihc.com ([10.50.128.46]) by
	gimail1.co.ihc.com with esmtp (Exim 4.62) (envelope-from
	<Greg.Snow@intermountainmail.org>) id 1HbOof-00030c-I3; Tue, 10 Apr
	2007 16:27:58 -0600
Received: from LP-EXCHVS07.CO.IHC.COM ([10.50.128.40]) by
	lp-exchfe02.CO.IHC.COM with Microsoft SMTPSVC(6.0.3790.1830); Tue, 10
	Apr 2007 16:27:51 -0600
X-MimeOLE: Produced By Microsoft Exchange V6.5
Content-class: urn:content-classes:message
MIME-Version: 1.0
Date: Tue, 10 Apr 2007 16:27:50 -0600
Message-ID: <07E228A5BE53C24CAD490193A7381BBB9328D6@LP-EXCHVS07.CO.IHC.COM>
In-Reply-To: <1115a2b00704101425l2c78141ax757b09b02d4f95a1@mail.gmail.com>
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
Thread-Topic: [R] Reasons to Use R
Thread-Index: Acd7ttVWrSNQeV4XTgugLhncgIEz4AAByGRw
From: "Greg Snow" <Greg.Snow@intermountainmail.org>
To: "Wensui Liu" <liuwensui@gmail.com>
X-OriginalArrivalTime: 10 Apr 2007 22:27:51.0279 (UTC)
	FILETIME=[799F1FF0:01C77BBF]
X-WSS-ID: 6A02D0CF1M82070881-01-01
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-MIME-Autoconverted: from quoted-printable to 8bit by hypatia.math.ethz.ch id
	l3AMS76I005597
X-Mailman-Approved-At: Wed, 11 Apr 2007 08:04:38 +0200
Cc: r-help@stat.math.ethz.ch, Lorenzo Isella <lorenzo.isella@gmail.com>
Subject: Re: [R] Reasons to Use R
X-BeenThere: r-help@stat.math.ethz.ch
X-Mailman-Version: 2.1.9
Precedence: list
List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=unsubscribe>
List-Archive: <https://stat.ethz.ch/pipermail/r-help>
List-Post: <mailto:r-help@stat.math.ethz.ch>
List-Help: <mailto:r-help-request@stat.math.ethz.ch?subject=help>
List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Sender: r-help-bounces@stat.math.ethz.ch
Errors-To: r-help-bounces@stat.math.ethz.ch
Status: O
Content-Length: 6535
Lines: 164

I think SAS has the database part built into it.  I have heard 2nd hand
of new statisticians going to work for a company and asking if they have
SAS, the reply is "Yes we use SAS for our database, does it do
statistics also?"  Also I heard something about SAS is no longer
considered an acronym, they like having it be just a name and don't want
the fact that one of the S's used to stand for statistics to scare away
companies that use it as a database.

Maybe someone more up on SAS can confirm or deny this.

Also one issue to always look at is central control versus ease of
extendability.  If you have a program that is completely under your
control and does one set of things, then extending it to a new model
(big data) is fairly straight forward.  R is the opposite end of the
spectrum with many contributers and many techniques.  Extending some
basic pieces to be very efficient with big data could be done easily,
but would break many other pieces.  Getting all the different packages
to conform to a single standard in a short amount of time would be near
impossible.

With R's flexibility, there are probably some problems that can be done
quicker with a proper use of biglm than with SAS and I expect that with
some more work and maturity the SQLiteDF package may start to rival SAS
as well on certain problems.  While SAS is a useful program and great at
certain things, there are some tecniques that I would not even attempt
using SAS that are fairly straigh forward in R (I remember seeing some
SAS code to do a bootstrap that included a datastep to read in and
extract information from a SAS output file, <<SHUDDER>>  SAS/ODS has
improved this, but I would much rather bootstrap in R/S-PLUS than
anything else).

Remember, everything is better than everything else given the right
comparison.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow@intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Wensui Liu [mailto:liuwensui@gmail.com] 
> Sent: Tuesday, April 10, 2007 3:26 PM
> To: Greg Snow
> Cc: Bi-Info (http://members.home.nl/bi-info); Gabor 
> Grothendieck; Lorenzo Isella; r-help@stat.math.ethz.ch
> Subject: Re: [R] Reasons to Use R
> 
> Greg,
> As far as I understand, SAS is more efficient handling large 
> data probably than S+/R. Do you have any idea why?
> 
> On 4/10/07, Greg Snow <Greg.Snow@intermountainmail.org> wrote:
> > > -----Original Message-----
> > > From: r-help-bounces@stat.math.ethz.ch 
> > > [mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of Bi-Info 
> > > (http://members.home.nl/bi-info)
> > > Sent: Monday, April 09, 2007 4:23 PM
> > > To: Gabor Grothendieck
> > > Cc: Lorenzo Isella; r-help@stat.math.ethz.ch
> > > Subject: Re: [R] Reasons to Use R
> >
> > [snip]
> >
> > > So what's the big deal about S using files instead of 
> memory like R. 
> > > I don't get the point. Isn't there enough swap space for S? (Who 
> > > cares
> > > anyway: it works, isn't it?) Or are there any problems with S and 
> > > large datasets? I don't get it. You use them, Greg. So you might 
> > > discuss that issue.
> > >
> > > Wilfred
> > >
> > >
> >
> > This is my understanding of the issue (not anything official).
> >
> > If you use up all the memory while in R, then the OS will start 
> > swapping memory to disk, but the OS does not know what 
> parts of memory 
> > correspond to which objects, so it is entirely possible 
> that the chunk 
> > swapped to disk contains parts of different data objects, 
> so when you 
> > need one of those objects again, everything needs to be 
> swapped back 
> > in.  This is very inefficient.
> >
> > S-PLUS occasionally runs into the same problem, but since 
> it does some 
> > of its own swapping to disk it can be more efficient by swapping 
> > single data objects (data frames, etc.).  Also, since S-PLUS is 
> > already saving everything to disk, it does not actually 
> need to do a 
> > full swap, it can just look and see that a particular data 
> frame has 
> > not been used for a while, know that it is already saved on 
> the disk, 
> > and unload it from memory without having to write it to disk first.
> >
> > The g.data package for R has some of this functionality of keeping 
> > data on the disk until needed.
> >
> > The better approach for large data sets is to only have some of the 
> > data in memory at a time and to automatically read just the 
> parts that 
> > you need.  So for big datasets it is recommended to have the actual 
> > data stored in a database and use one of the database connection 
> > packages to only read in the subset that you need.  The SQLiteDF 
> > package for R is working on automating this process for R.  
> There are 
> > also the bigdata module for S-PLUS and the biglm package for R have 
> > ways of doing some of the common analyses using chunks of data at a 
> > time.  This idea is not new.  There was a program in the late 1970s 
> > and 80s called Rummage by Del Scott (I guess technically it 
> still exists, I have a copy on a 5.25"
> > floppy somewhere) that used the approach of specify the model you 
> > wanted to fit first, then specify the data file.  Rummage 
> would then 
> > figure out which sufficient statistics were needed and read 
> the data 
> > in chunks, compute the sufficient statistics on the fly, 
> and not keep 
> > more than a couple of lines of the data in memory at once.  
> > Unfortunately it did not have much of a user interface, so 
> when memory 
> > was cheap and datasets only medium sized it did not compete well, I 
> > guess it was just a bit too ahead of its time.
> >
> > Hope this helps,
> >
> >
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow@intermountainmail.org
> > (801) 408-8111
> >
> > ______________________________________________
> > R-help@stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> --
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
>

______________________________________________
R-help@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

