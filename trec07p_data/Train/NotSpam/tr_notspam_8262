From r-help-bounces@stat.math.ethz.ch  Mon May  7 12:45:54 2007
Return-Path: <r-help-bounces@stat.math.ethz.ch>
Received: from hypatia.math.ethz.ch (hypatia.math.ethz.ch [129.132.145.15])
	by flax9.uwaterloo.ca (8.12.8/8.12.5) with ESMTP id l47GjqqD001085
	for <ktwarwic@flax9.uwaterloo.ca>; Mon, 7 May 2007 12:45:53 -0400
Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l47GiBWh011673;
	Mon, 7 May 2007 18:44:31 +0200
X-Spam-Checker-Version: SpamAssassin 3.1.8 (2007-02-13) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, score=-0.4 required=5.0 tests=AWL autolearn=no version=3.1.8
Received: from mtaprod2.gene.com (smtp-out.gene.com [72.34.128.225])
	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l47GcROQ009424
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL)
	for <r-help@stat.math.ethz.ch>; Mon, 7 May 2007 18:38:28 +0200
Received: from mta-rwc-1.gene.com (mta-rwc-1.gene.com [128.137.15.60])
	by mtaprod2.gene.com (Switch-3.2.4/Switch-3.2.4) with ESMTP id
	l47GcNT1024800
	(version=TLSv1/SSLv3 cipher=EDH-RSA-DES-CBC3-SHA bits=168 verify=OK);
	Mon, 7 May 2007 09:38:24 -0700
Received: from BGUNTERDT2 (dhcp144-77.gene.com [128.137.144.77])
	by mta-rwc-1.gene.com (Switch-3.1.7/Switch-3.1.7) with ESMTP id
	l47GcNLv004778; Mon, 7 May 2007 09:38:23 -0700 (PDT)
From: Bert Gunter <gunter.berton@gene.com>
To: "'hadley wickham'" <h.wickham@gmail.com>,
   "'Wensui Liu'" <liuwensui@gmail.com>
Date: Mon, 7 May 2007 09:38:23 -0700
Organization: Genentech Inc.
Message-ID: <003d01c790c6$20f41560$4d908980@gne.windows.gene.com>
MIME-Version: 1.0
X-Mailer: Microsoft Office Outlook 11
Thread-Index: AceQoyW0eT+MBr/mR7mGwUE4iOc7wgAHch+A
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1896
In-Reply-To: <f8e6ff050705070526o383ab841gb8f45285b1bcd74@mail.gmail.com>
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
Cc: r-help@stat.math.ethz.ch
Subject: Re: [R] Neural Nets (nnet) - evaluating success rate of predictions
X-BeenThere: r-help@stat.math.ethz.ch
X-Mailman-Version: 2.1.9
Precedence: list
List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=unsubscribe>
List-Archive: <https://stat.ethz.ch/pipermail/r-help>
List-Post: <mailto:r-help@stat.math.ethz.ch>
List-Help: <mailto:r-help-request@stat.math.ethz.ch?subject=help>
List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request@stat.math.ethz.ch?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Sender: r-help-bounces@stat.math.ethz.ch
Errors-To: r-help-bounces@stat.math.ethz.ch

Folks:

If I understand correctly, the following may be pertinent.

Note that the procedure:

min.nnet = nnet[k] such that error rate of nnet[k] = min[i] {error
rate(nnet(training data) from ith random start) }

does not guarantee a classifier with a lower error rate on **new** data than
any single one of the random starts. That is because you are using the same
training set to choose the model (= nnet parameters) as you are using to
determine the error rate. I know it's tempting to think that choosing the
best among many random starts always gets you a better classifier, but it
need not. The error rate on the training set for any classifier -- be it a
single one or one derived in some way from many -- is a biased estimate of
the true error rate, so that choosing a classifer on this basis does not
assure better performance for future data. In particular, I would guess that
choosing the best among many (hundreds/thousands) random starts is probably
almost guaranteed to produce a poor predictor (ergo the importance of
parsimony/penalization).  I would appreciate comments from anyone, pro or
con, with knowledge and experience of these things, however, as I'm rather
limited on both.

The simple answer to the question of obtaining the error rate using
validation data is: Do whatever you like to choose/fit a classifier on the
training set. **Once you are done,** the estimate of your error rate is the
error rate you get on applying that classifier to the validation set. But
you can do this only once! If you don't like that error rate and go back to
finding a a better predictor in some way, then your validation data have now
been used to derive the classifier and thus has become part of the training
data, so any further assessment of the error rate of a new classifier on it
is now also a biased estimate. You need yet new validation data for that.

Of course, there are all sort of cross validation schemes one can use to
avoid -- or maybe mitigate -- these issues: most books on statistical
classification/machine learning discuss this in detail.


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces@stat.math.ethz.ch
[mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of hadley wickham
Sent: Monday, May 07, 2007 5:26 AM
To: Wensui Liu
Cc: r-help@stat.math.ethz.ch
Subject: Re: [R] Neural Nets (nnet) - evaluating success rate of predictions

Pick the one with the lowest error rate on your training data?
Hadley

On 5/7/07, Wensui Liu <liuwensui@gmail.com> wrote:
> well, how to do you know which ones are the best out of several hundreds?
> I will average all results out of several hundreds.
>
> On 5/7/07, hadley wickham <h.wickham@gmail.com> wrote:
> > On 5/6/07, nathaniel Grey <nathaniel.grey@yahoo.co.uk> wrote:
> > > Hello R-Users,
> > >
> > > I have been using (nnet) by Ripley  to train a neural net on a test
dataset, I have obtained predictions for a validtion dataset using:
> > >
> > > PP<-predict(nnetobject,validationdata)
> > >
> > > Using PP I can find the -2 log likelihood for the validation datset.
> > >
> > > However what I really want to know is how well my nueral net is doing
at classifying my binary output variable. I am new to R and I can't figure
out how you can assess the success rates of predictions.
> > >
> >
> > table(PP, binaryvariable)
> > should get you started.
> >
> > Also if you're using nnet with random starts, I strongly suggest
> > taking the best out of several hundred (or maybe thousand) trials - it
> > makes a big difference!
> >
> > Hadley
> >
> > ______________________________________________
> > R-help@stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
>

______________________________________________
R-help@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

